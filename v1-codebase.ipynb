{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langgraph.graph import StateGraph, MessagesState, START\n",
    "from typing import Literal\n",
    "\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple, Dict, Any\n",
    "from langchain.docstore.document import Document\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "from paperscraper.pubmed import get_and_dump_pubmed_papers\n",
    "from paperscraper.utils import load_jsonl\n",
    "from paperscraper.pdf import save_pdf_from_dump\n",
    "from IPython.display import display, Image\n",
    "\n",
    "import utils\n",
    "import pymupdf\n",
    "from emulation_utils import *\n",
    "\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "from trialist_utils import *\n",
    "\n",
    "# for the clinical trial parser\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(\"./trialistUtils\"))\n",
    "from trialistUtils import parse_clinical_trial\n",
    "\n",
    "# Read variables from .env file into the environment\n",
    "env_file = '.env'\n",
    "if os.path.exists(env_file):\n",
    "    with open(env_file) as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('=')\n",
    "            if len(parts) == 2:\n",
    "                key, value = parts\n",
    "                print(f\"Setting {key} to {value}\")\n",
    "                os.environ[key] = value\n",
    "\n",
    "\n",
    "from dataframe_utils import Informatician as InformaticianTool\n",
    "from dataframe_utils import clean_ec\n",
    "\n",
    "from llm_zoo_rlhf import LLMZoo, GPT4AzureModel, OllamaModel, OpenaiClient, LLMwithReward, LLMwithInteractive\n",
    "import logging\n",
    "logging.getLogger(\"matplotlib\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"sentence_transformers\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"httpcore\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"httpx\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"paperscraper\").setLevel(logging.CRITICAL)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set trial id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRIAL_ID = 'NCT04134403'\n",
    "# TRIAL_ID = 'NCT02856698'\n",
    "# TRIAL_ID = 'NCT06091982'\n",
    "# TRIAL_ID = 'NCT00475852'\n",
    "# TRIAL_ID = 'NCT03872011'\n",
    "TRIAL_ID = 'NCT04691505'\n",
    "\n",
    "assert TRIAL_ID in ['NCT04134403', 'NCT02856698', 'NCT06091982', 'NCT00475852', 'NCT03872011', 'NCT04691505'] or TRIAL_ID in ['NCT00000011', 'NCT00000012'], f\"TRIAL_ID {TRIAL_ID} is not in the list of valid trial IDs.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_MODEL = 'phi4'\n",
    "# LLM_MODEL = 'deepseek'\n",
    "# LLM_MODEL = 'gpt4o'\n",
    "# LLM_MODEL = 'gpt4openai'\n",
    "# LLM_MODEL = 'gemma3'\n",
    "\n",
    "assert LLM_MODEL in ['phi4', 'deepseek', 'gpt4o', 'gpt4openai', 'gemma3']\n",
    "print('llm model:', LLM_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining large language model\n",
    "# Example usage\n",
    "llm = LLMZoo()\n",
    "\n",
    "# openai gpt4 api\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "openai_organization = os.getenv('OPENAI_ORGANIZATION')\n",
    "openai_project = os.getenv('OPENAI_PROJECT')\n",
    "\n",
    "# Azure gpt4o api\n",
    "api_key = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "api_version = \"2023-05-15\"\n",
    "assert api_key and endpoint and api_key\n",
    "\n",
    "# Register models with initialization parameters\n",
    "# llm.register_model(\"GPT-4o\", GPT4AzureModel(api_key=api_key, endpoint=endpoint, api_version=api_version))\n",
    "# llm.select_model(\"GPT-4o\")\n",
    "\n",
    "# llm.register_model(\"OllamaModel\", OllamaModel(model='deepseek-r1:14b', temperature=0.0))\n",
    "llm.register_model('phi4', OllamaModel(model='phi4', temperature=0.0))\n",
    "llm.register_model('gemma3', OllamaModel(model='gemma3:27b', temperature=0.0))\n",
    "llm.register_model('deepseek', OllamaModel(model='deepseek-r1:14b', temperature=0.0))\n",
    "if api_key and endpoint and api_version:\n",
    "    llm.register_model(\"gpt4o\", GPT4AzureModel(api_key=api_key, endpoint=endpoint, api_version=api_version))\n",
    "if openai_api_key and openai_organization and openai_project:\n",
    "    llm.register_model(\"gpt4openai\", OpenaiClient(api_key=openai_api_key, organization=openai_organization, project=openai_project))\n",
    "\n",
    "llm.select_model(LLM_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_pdfs_pubmed = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic search Model initialization\n",
    "EMBEDDING_MODEL_NAME ='sentence-transformers/all-MiniLM-L6-v2'\n",
    "encoder = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "## FOR CLINICIAN AGENT ##\n",
    "def create_rag_knowledge_base_local(literature_folders, return_docs=False):\n",
    "    pdf_paths = []\n",
    "    for literature_folder in literature_folders:\n",
    "        # Get all PDF files in the folder and its subfolders\n",
    "        for root, _, files in os.walk(literature_folder):\n",
    "            for file in files:\n",
    "                if file.endswith('.pdf'):\n",
    "                    pdf_paths.append(os.path.join(root, file))\n",
    "\n",
    "    pdf_strings = []\n",
    "\n",
    "    for pdf_path in pdf_paths:\n",
    "        try:\n",
    "            doc = pymupdf.open(pdf_path)\n",
    "            keep_pages = None\n",
    "            remove_pages = None\n",
    "            pdf_string, _, _, _, _ = utils.create_extraction_components(doc, keep_pages=keep_pages, remove_pages=remove_pages, run_llms=0)\n",
    "            pdf_strings.append(pdf_string)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    RAW_KNOWLEDGE_BASE = [\n",
    "        Document(page_content=pdf_string)\n",
    "        for pdf_string in pdf_strings\n",
    "    ]\n",
    "\n",
    "    # Splitting documents into chunks\n",
    "    docs_processed = utils.split_documents(\n",
    "        256,  # We choose a chunk size adapted to our model\n",
    "        RAW_KNOWLEDGE_BASE,\n",
    "        tokenizer_name=EMBEDDING_MODEL_NAME,\n",
    "    )\n",
    "\n",
    "    docs_content = []\n",
    "    for doc_processed in docs_processed:\n",
    "        docs_content.append(doc_processed.page_content)\n",
    "\n",
    "    # If no documents are found, return an empty DataFrame and index\n",
    "\n",
    "    if return_docs:\n",
    "        return docs_content\n",
    "\n",
    "    if len(docs_content) == 0:\n",
    "        return pd.DataFrame(), faiss.IndexFlatL2(0)\n",
    "\n",
    "    # Encoding the documents\n",
    "    encoder = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "    vectors = encoder.encode(docs_content)\n",
    "    df_database = pd.DataFrame(data=docs_content, columns=['text'])\n",
    "    vector_dimension = vectors.shape[1]\n",
    "    index = faiss.IndexFlatL2(vector_dimension)\n",
    "    faiss.normalize_L2(vectors)\n",
    "    index.add(vectors)\n",
    "    return df_database, index\n",
    "\n",
    "\n",
    "def create_rag_knowledge_base_pubmed(question, return_docs=False):\n",
    "    docs_content = []\n",
    "\n",
    "    ### Pubmed data\n",
    "    prompt = f\"\"\"\n",
    "    You are a keyword extraction assistant. The output MUST only be \"ANS:\" followed by a comma-separated list of keywords. \\\n",
    "    Be concise.\n",
    "\n",
    "    The keywords should be related to biomedicine and biology. The keywords will be used to parse relevant literature to answer the question.\n",
    "    Do NOT use keywords related to statistics or data analysis.\n",
    "\n",
    "    Provide at most 3 keywords from the question below.\n",
    "    Question: {question}\n",
    "\n",
    "    Expected Structure Output:\n",
    "    ANS: keyword1, keyword2, keyword3\n",
    "    \"\"\"\n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    if \"ANS\" in response:\n",
    "        response = response.split(\"ANS:\")[1].split(\"\\n\")[0]\n",
    "    if \"<|eot_id|>\" in response:\n",
    "        response = response.split(\"<|eot_id|>\")[0]\n",
    "    sep_keywords = response.split(\",\")\n",
    "    for i in range(len(sep_keywords)): sep_keywords[i] = sep_keywords[i].strip()\n",
    "    query_paperscraper = []\n",
    "    for keyword in sep_keywords: query_paperscraper.append([keyword])\n",
    "    get_and_dump_pubmed_papers(query_paperscraper, output_filepath='temp/pubmed_result.jsonl')\n",
    "    abstracts = []\n",
    "    dates = []\n",
    "    for paper in load_jsonl('temp/pubmed_result.jsonl'):\n",
    "        if paper['abstract'] is not None:\n",
    "            abstracts.append(paper['abstract'])\n",
    "            dates.append(paper['date'])\n",
    "\n",
    "    # If no abstracts are found, return an empty DataFrame and index\n",
    "    if len(abstracts) == 0:\n",
    "        return pd.DataFrame(), faiss.IndexFlatL2(0)\n",
    "\n",
    "    # Make RAG_KNOWLEDGE_BASE with all abstracts\n",
    "    RAW_KNOWLEDGE_BASE = []\n",
    "    # Sort the abstracts by date, most recent first\n",
    "    abstracts = [x for _, x in sorted(zip(dates, abstracts), key=lambda pair: pair[0], reverse=True)]\n",
    "    if len(abstracts) != 0:\n",
    "        # Get the first 500 abstracts\n",
    "        abstracts = abstracts[:500]\n",
    "        for abstract in abstracts: RAW_KNOWLEDGE_BASE.append(Document(page_content=abstract))\n",
    "\n",
    "        # Splitting documents into chunks\n",
    "        docs_processed = utils.split_documents(\n",
    "            256,  # We choose a chunk size adapted to our model\n",
    "            RAW_KNOWLEDGE_BASE,\n",
    "            tokenizer_name=EMBEDDING_MODEL_NAME,\n",
    "        )\n",
    "\n",
    "        for doc_processed in docs_processed: docs_content.append(doc_processed.page_content)\n",
    "\n",
    "    if return_docs:\n",
    "        return docs_content, sep_keywords\n",
    "\n",
    "    # Encoding the documents\n",
    "    encoder = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "    vectors = encoder.encode(docs_content)\n",
    "    df_database = pd.DataFrame(data=docs_content, columns=['text'])\n",
    "    vector_dimension = vectors.shape[1]\n",
    "    index = faiss.IndexFlatL2(vector_dimension)\n",
    "    faiss.normalize_L2(vectors)\n",
    "    index.add(vectors)\n",
    "    return df_database, index, sep_keywords\n",
    "\n",
    "\n",
    "def get_relevant_info_from_lit(question, df_database_user, index_user):\n",
    "    # Create the RAG knowledge base\n",
    "    # df_database, index = create_rag_knowledge_base() ## theoretically this should be done once and then the index should be saved and loaded\n",
    "    try:\n",
    "        df_database, index = create_rag_knowledge_base_pubmed(question)\n",
    "    except:\n",
    "        pass\n",
    "    # If the database is empty, return an empty string\n",
    "    if df_database.empty:\n",
    "        return \"Knowledge base is empty, cannot perform search.\"\n",
    "    search_vector = encoder.encode(question)\n",
    "    _vector = np.array([search_vector])\n",
    "    faiss.normalize_L2(_vector)\n",
    "    k = index.ntotal\n",
    "    distances, ann = index.search(_vector, k=k)\n",
    "    results = pd.DataFrame({'distances': distances[0], 'ann': ann[0]})\n",
    "    merge = pd.merge(results, df_database, left_on='ann', right_index=True)\n",
    "    # Only getting the top 5 results\n",
    "    combined_texts = (\"\\n\\n\").join(list(merge['text'].values[0:5]))\n",
    "    return combined_texts\n",
    "\n",
    "\n",
    "def create_rag_knowledge_base_ibkh_triplets(df_ibkh_triplets):\n",
    "    docs_content = list(df_ibkh_triplets['full_text'].values)\n",
    "\n",
    "    # If no documents are found, return an empty DataFrame and index\n",
    "    if len(docs_content) == 0:\n",
    "        return pd.DataFrame(), faiss.IndexFlatL2(0)\n",
    "\n",
    "    # Encoding the documents\n",
    "    encoder = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "    vectors = encoder.encode(docs_content)\n",
    "    df_database = pd.DataFrame(data=docs_content, columns=['text'])\n",
    "    vector_dimension = vectors.shape[1]\n",
    "    index = faiss.IndexFlatL2(vector_dimension)\n",
    "    faiss.normalize_L2(vectors)\n",
    "    index.add(vectors)\n",
    "    return df_database, index\n",
    "\n",
    "\n",
    "def get_relevant_info_from_lit_and_pubmed(state, top_k=5):\n",
    "    \"\"\"\n",
    "    Combines knowledge from local files and PubMed to find relevant information.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The user's question.\n",
    "        top_k (int): The number of top results to return.\n",
    "        \n",
    "    Returns:\n",
    "        str: A combined string of the most relevant text snippets.\n",
    "    \"\"\"\n",
    "    \n",
    "    question = state['ongoing_question']\n",
    "\n",
    "    # 1. Get knowledge from PubMed\n",
    "    pubmed_abstracts_docs, sep_keywords = create_rag_knowledge_base_pubmed(question, return_docs=True)\n",
    "\n",
    "    # 2. Get knowledge from local files\n",
    "    if extract_pdfs_pubmed:\n",
    "        save_pdf_from_dump('temp/pubmed_result.jsonl', pdf_path='Data/Pubmed/.', key_to_save='doi')\n",
    "        pdf_docs = create_rag_knowledge_base_local([\"Data/Sepsis\", \"Data/Pubmed\"], return_docs=True)\n",
    "        # delete all files in Data/Pubmed\n",
    "        for filename in os.listdir(\"Data/Pubmed\"):\n",
    "            file_path = os.path.join(\"Data/Pubmed\", filename)\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "    else:\n",
    "        pdf_docs = create_rag_knowledge_base_local([\"Data/Sepsis\"], return_docs=True)\n",
    "\n",
    "    # remove pubmed_results.jsonl file if it exists\n",
    "    if os.path.exists('temp/pubmed_result.jsonl'):\n",
    "        os.remove('temp/pubmed_result.jsonl')\n",
    "\n",
    "    # 3. Combine the databases and indices\n",
    "    all_docs_content = []\n",
    "    if len(pdf_docs) > 0:\n",
    "        all_docs_content.extend(list(pdf_docs))\n",
    "    if len(pubmed_abstracts_docs) > 0:\n",
    "        all_docs_content.extend(list(pubmed_abstracts_docs))\n",
    "    if not state['variables']['compiled_relevant_texts'].empty:\n",
    "        all_docs_content.extend(state['variables']['compiled_relevant_texts']['text'].tolist())\n",
    "\n",
    "    # remove all duplicates from all_docs_content\n",
    "    all_docs_content = list(set(all_docs_content))\n",
    "\n",
    "    if len(all_docs_content) == 0:\n",
    "        return \"Knowledge base is empty, cannot perform search.\"\n",
    "\n",
    "    # Re-encode and create a new, unified index\n",
    "    vectors = encoder.encode(all_docs_content)\n",
    "    combined_df = pd.DataFrame(data=all_docs_content, columns=['text'])\n",
    "    \n",
    "    vector_dimension = vectors.shape[1]\n",
    "    combined_index = faiss.IndexFlatL2(vector_dimension)\n",
    "    faiss.normalize_L2(vectors)\n",
    "    combined_index.add(vectors)\n",
    "    \n",
    "    # 4. Search the combined index\n",
    "    search_vector = encoder.encode(question)\n",
    "    _vector = np.array([search_vector])\n",
    "    faiss.normalize_L2(_vector)\n",
    "    \n",
    "    # Ensure k is not greater than the number of items in the index\n",
    "    k = min(top_k, combined_index.ntotal)\n",
    "    if k == 0:\n",
    "        return \"Knowledge base is empty, cannot perform search.\"\n",
    "\n",
    "    distances, ann = combined_index.search(_vector, k=k)\n",
    "    \n",
    "    results = pd.DataFrame({'distances': distances[0], 'ann': ann[0]})\n",
    "    merge = pd.merge(results, combined_df, left_on='ann', right_index=True)\n",
    "    \n",
    "    # 5. Format and return the results\n",
    "    combined_texts = \"\\n\\n---\\n\\n\".join(merge['text'].tolist())\n",
    "\n",
    "    state['variables']['compiled_relevant_texts'] = pd.concat([state['variables']['compiled_relevant_texts'], merge[['text']]], ignore_index=True).reset_index(drop=True)\n",
    "    state['variables']['compiled_relevant_texts'].drop_duplicates(subset=['text'], inplace=True)\n",
    "\n",
    "    return combined_texts, state\n",
    "\n",
    "\n",
    "def get_relevant_info_from_lit_and_pubmed_ibkh(state, top_k=5):\n",
    "    \"\"\"\n",
    "    Combines knowledge from local files and PubMed to find relevant information.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The user's question.\n",
    "        top_k (int): The number of top results to return.\n",
    "        \n",
    "    Returns:\n",
    "        str: A combined string of the most relevant text snippets.\n",
    "    \"\"\"\n",
    "    \n",
    "    question = state['ongoing_question']\n",
    "\n",
    "    # 1. Get knowledge from PubMed\n",
    "    pubmed_abstracts_docs, sep_keywords = create_rag_knowledge_base_pubmed(question, return_docs=True)\n",
    "\n",
    "    # 2. Get knowledge from local files\n",
    "    if extract_pdfs_pubmed:\n",
    "        save_pdf_from_dump('temp/pubmed_result.jsonl', pdf_path='Data/Pubmed/.', key_to_save='doi')\n",
    "        pdf_docs = create_rag_knowledge_base_local([\"Data/Sepsis\", \"Data/Pubmed\"], return_docs=True)\n",
    "        # delete all files in Data/Pubmed\n",
    "        for filename in os.listdir(\"Data/Pubmed\"):\n",
    "            file_path = os.path.join(\"Data/Pubmed\", filename)\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "    else:\n",
    "        pdf_docs = create_rag_knowledge_base_local([\"Data/Sepsis\"], return_docs=True)\n",
    "\n",
    "    # remove pubmed_results.jsonl file if it exists\n",
    "    if os.path.exists('temp/pubmed_result.jsonl'):\n",
    "        os.remove('temp/pubmed_result.jsonl')\n",
    "\n",
    "    # 3. Combine the databases and indices\n",
    "    all_docs_content = []\n",
    "    if len(pdf_docs) > 0:\n",
    "        all_docs_content.extend(list(pdf_docs))\n",
    "    if len(pubmed_abstracts_docs) > 0:\n",
    "        all_docs_content.extend(list(pubmed_abstracts_docs))\n",
    "    if not df_ibkh_triplets.empty:\n",
    "        all_docs_content.extend(df_ibkh_triplets['full_text'].tolist())\n",
    "    if not state['variables']['compiled_relevant_texts'].empty:\n",
    "        all_docs_content.extend(state['variables']['compiled_relevant_texts']['text'].tolist())\n",
    "\n",
    "    # remove all duplicates from all_docs_content\n",
    "    all_docs_content = list(set(all_docs_content))\n",
    "\n",
    "    if not all_docs_content:\n",
    "        return \"Could not find any relevant information.\"\n",
    "\n",
    "    # Re-encode and create a new, unified index\n",
    "    vectors = encoder.encode(all_docs_content)\n",
    "    combined_df = pd.DataFrame(data=all_docs_content, columns=['text'])\n",
    "    \n",
    "    vector_dimension = vectors.shape[1]\n",
    "    combined_index = faiss.IndexFlatL2(vector_dimension)\n",
    "    faiss.normalize_L2(vectors)\n",
    "    combined_index.add(vectors)\n",
    "    \n",
    "    # 4. Search the combined index\n",
    "    search_vector = encoder.encode(question)\n",
    "    _vector = np.array([search_vector])\n",
    "    faiss.normalize_L2(_vector)\n",
    "    \n",
    "    # Ensure k is not greater than the number of items in the index\n",
    "    k = min(top_k, combined_index.ntotal)\n",
    "    if k == 0:\n",
    "        return \"Knowledge base is empty, cannot perform search.\"\n",
    "\n",
    "    distances, ann = combined_index.search(_vector, k=k)\n",
    "    \n",
    "    results = pd.DataFrame({'distances': distances[0], 'ann': ann[0]})\n",
    "    merge = pd.merge(results, combined_df, left_on='ann', right_index=True)\n",
    "    \n",
    "    # 5. Format and return the results\n",
    "    combined_texts = \"\\n\\n---\\n\\n\".join(merge['text'].tolist())\n",
    "\n",
    "    state['variables']['compiled_relevant_texts'] = pd.concat([state['variables']['compiled_relevant_texts'], merge[['text']]], ignore_index=True).reset_index(drop=True)\n",
    "    state['variables']['compiled_relevant_texts'].drop_duplicates(subset=['text'], inplace=True)\n",
    "\n",
    "    return combined_texts, state\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "## FOR MAIN SUPERVISOR AGENT ##\n",
    "\n",
    "def get_most_relevant_clinical_trials(user_question):\n",
    "    # Clean up the user question to only talk about the treatment and disease and other biological information\n",
    "    prompt = f\"\"\"\n",
    "    You are given a USER QUESTION.\n",
    "\n",
    "    USER QUESTION: {user_question}\n",
    "\n",
    "    Your task is to **extract only the core biological content** from the USER QUESTION.\n",
    "\n",
    "    ### Keep:\n",
    "    - Treatment (drug, intervention, biomarker, etc.)\n",
    "    - Disease/condition\n",
    "    - Outcome (if relevant)\n",
    "\n",
    "    ### Remove:\n",
    "    - Any mention of study design, methodology, instructions, protocols, trial setup\n",
    "    - Generic phrases like \"design a trial,\" \"develop a study,\" or \"assess impact\"\n",
    "\n",
    "    Return only the cleaned-up question in the format:\n",
    "\n",
    "    CLEANED USER QUESTION: <cleaned-up question>\n",
    "\n",
    "    Example Input / Outpus:\n",
    "    \"What study design should be used to assess the impact of metformin on glucose levels in prediabetic adults?\"\n",
    "    CLEANED USER QUESTION: Does metformin lower glucose in prediabetic adults?\n",
    "\n",
    "    \"Develop an observational study to determine if aspirin reduces stroke risk in elderly patients.\"\n",
    "    CLEANED USER QUESTION: Does aspirin reduce stroke risk in elderly patients?\n",
    "\n",
    "    Do not add any extra words, explanations, or formatting.\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    cleaned_user_question = response.split(\"CLEANED USER QUESTION:\")[1].strip()\n",
    "\n",
    "\n",
    "    # Prompt to get structured keywords of disease and treatment from user question in Essie expression syntax\n",
    "    prompt = f\"\"\"\n",
    "    You are given a USER QUESTION. \n",
    "\n",
    "    USER QUESTION: {cleaned_user_question}\n",
    "\n",
    "    Your task is to extract the keywords of disease and treatment from the USER QUESTION in Essie expression syntax.\n",
    "\n",
    "    If there is an outcome, extract the keywords of the outcome as well.\n",
    "\n",
    "    If there is no outcome, should return OUTCOME: None\n",
    "\n",
    "    The treatment keywords should not include \"treatment\" in them.\n",
    "\n",
    "    Return the keywords in the expected structure output:\n",
    "    DISEASE: <disease keywords>; TREATMENT: <treatment keywords>; OUTCOME: <outcome keywords>\n",
    "\n",
    "    Do not provide any additional information or explanation.\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    # Parse disease and treatment keywords from the response\n",
    "    disease_str = response.split(\"DISEASE: \")[1].split(\";\")[0].strip()\n",
    "    trt_str = response.split(\"TREATMENT: \")[1].split(\";\")[0].strip()\n",
    "    disease_keywords = disease_str.split(\",\")\n",
    "    trt_keywords = trt_str.split(\",\")\n",
    "    outcome_str = response.split(\"OUTCOME: \")[1].strip()\n",
    "    outcome_keywords = outcome_str.split(\",\")\n",
    "    if outcome_str == \"None\":\n",
    "        outcome_keywords = None\n",
    "    # remove periods from keywords\n",
    "    disease_keywords = [keyword.replace(\".\", \"\") for keyword in disease_keywords]\n",
    "    trt_keywords = [keyword.replace(\".\", \"\") for keyword in trt_keywords]\n",
    "    essex_disease_str = \" AND \".join([f\"{keyword.strip()}\" for keyword in disease_keywords])\n",
    "    essex_trt_str = \" AND \".join([f\"{keyword.strip()}\" for keyword in trt_keywords])\n",
    "    essex_outcome_str = \" AND \".join([f\"{keyword.strip()}\" for keyword in outcome_keywords]) if outcome_keywords else None\n",
    "    df_clinical = utils.get_clinical_trials_data(essex_disease_str, essex_trt_str, other_keywords_str=essex_outcome_str)\n",
    "\n",
    "\n",
    "    # Get the most similar trials\n",
    "    nct_ids = df_clinical['NCT ID'].values.tolist()\n",
    "    clinical_trials = df_clinical['string'].values.tolist()\n",
    "\n",
    "    EMBEDDING_MODEL_NAME ='sentence-transformers/all-MiniLM-L6-v2'\n",
    "    encoder = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "    docs_content = []\n",
    "    spliced_nct_ids = []\n",
    "\n",
    "    RAW_KNOWLEDGE_BASE = []\n",
    "    if len(clinical_trials) != 0:\n",
    "        # Get the first 500 abstracts\n",
    "        for i, clinical_trial in enumerate(clinical_trials):\n",
    "            RAW_KNOWLEDGE_BASE.append(Document(page_content=clinical_trial, metadata={\"source\": nct_ids[i]}))\n",
    "\n",
    "        # Splitting documents into chunks\n",
    "        docs_processed = utils.split_documents(\n",
    "            256,  # We choose a chunk size adapted to our model\n",
    "            RAW_KNOWLEDGE_BASE,\n",
    "            tokenizer_name=EMBEDDING_MODEL_NAME,\n",
    "        )\n",
    "\n",
    "        for doc_processed in docs_processed:\n",
    "            docs_content.append(doc_processed.page_content)\n",
    "            spliced_nct_ids.append(doc_processed.metadata[\"source\"])\n",
    "\n",
    "    # Encoding the documents\n",
    "    encoder = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "    vectors = encoder.encode(docs_content)\n",
    "    df_database = pd.DataFrame(data=docs_content, columns=['text'])\n",
    "    df_database['NCT ID'] = spliced_nct_ids\n",
    "    vector_dimension = vectors.shape[1]\n",
    "    index = faiss.IndexFlatL2(vector_dimension)\n",
    "    faiss.normalize_L2(vectors)\n",
    "    index.add(vectors)\n",
    "\n",
    "    search_vector = encoder.encode(cleaned_user_question)\n",
    "    _vector = np.array([search_vector])\n",
    "    faiss.normalize_L2(_vector)\n",
    "    k = index.ntotal\n",
    "    distances, ann = index.search(_vector, k=k)\n",
    "    results = pd.DataFrame({'distances': distances[0], 'ann': ann[0]})\n",
    "    merge = pd.merge(results, df_database, left_on='ann', right_index=True)\n",
    "\n",
    "    # Get the first 3 unique NCT IDs\n",
    "    unique_nct_ids = merge['NCT ID'].unique().tolist()\n",
    "    # Take the first 3 unique NCT IDs\n",
    "    closest_nct_ids = unique_nct_ids[:5]\n",
    "    return closest_nct_ids, closest_nct_ids[0]\n",
    "\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "## FOR STATISTICIAN AGENT ##\n",
    "def build_trial_emulation_protocol(state: dict):\n",
    "    \"\"\"\n",
    "    Constructs a structured trial emulation protocol using key information from the state.\n",
    "    \"\"\"\n",
    "    # Extract key trial components from the state for a more focused prompt\n",
    "    trial_info = {\n",
    "        \"Inclusion Criteria\": state['variables'].get('inclusion_criteria', 'Not specified'),\n",
    "        \"Exclusion Criteria\": state['variables'].get('exclusion_criteria', 'Not specified'),\n",
    "        \"Treatment Definition\": state['variables'].get('treatment_definition', 'Not specified'),\n",
    "        \"Outcome Definition\": state['variables']['outcome_definitions'][state['variables']['current_outcome_index']],\n",
    "    }\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a statistician tasked with drafting a clear and structured trial emulation protocol.\n",
    "\n",
    "    **INSTRUCTIONS:**\n",
    "    Based on the provided KEY TRIAL COMPONENTS, write a formal trial emulation protocol. The protocol should logically connect these components and briefly outline the analysis plan. It must include the following sections:\n",
    "    1.  **Objective:** A single sentence stating the goal of the study.\n",
    "    2.  **Study Population:** Detail the inclusion and exclusion criteria.\n",
    "    3.  **Intervention and Comparator:** Describe the treatment being emulated.\n",
    "    4.  **Outcome(s):** Specify the primary and any secondary outcomes.\n",
    "    5.  **Analysis Plan:** Briefly mention the intended statistical approach (e.g., \"A causal inference model will be used to estimate the treatment effect after adjusting for confounders.\").\n",
    "\n",
    "    **KEY TRIAL COMPONENTS:**\n",
    "    {json.dumps(trial_info, indent=2)}\n",
    "\n",
    "    **FORMATTING:**\n",
    "    - Use clear headings for each section.\n",
    "    - Provide ONLY the protocol text as the answer.\n",
    "\n",
    "    **ANSWER:**\n",
    "    \"\"\"\n",
    "    state['answering_agents'].append(\"Statistician\")\n",
    "    state['prompts'].append(prompt)\n",
    "    response = llm.invoke(prompt, use_cache=False)\n",
    "    state['responses'].append(response)\n",
    "    \n",
    "    print(state['answering_agents'][-1])\n",
    "    print(state['responses'][-1])\n",
    "    print(\"------------------------------------------\")\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    protocol = response\n",
    "    return protocol\n",
    "\n",
    "\n",
    "def select_methods(state, df, trial_protocol, list_of_treatment, list_of_outcome, list_of_duration):\n",
    "    \"\"\"\n",
    "    Selects the best methods based on the dataset and trial protocol using a more structured prompt and robust parsing.\n",
    "    \"\"\"\n",
    "    # Create a summary of key dataset characteristics for the prompt\n",
    "    dataset_summary = {\n",
    "        \"Number of Patients\": df.shape[0],\n",
    "        \"Number of Covariates\": df.shape[1],\n",
    "        \"Treatment Column\": list_of_treatment[0],\n",
    "        \"Outcome Column\": list_of_outcome[0], # Assuming primary outcome is first\n",
    "        \"Duration Column\": list_of_duration[0] # Assuming primary duration is first\n",
    "    }\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert statistician. Your task is to select the most appropriate statistical methods for a trial emulation study based on the provided protocol and data summary.\n",
    "\n",
    "    **CONTEXT:**\n",
    "    - **Trial Emulation Protocol:** {trial_protocol}\n",
    "    - **Dataset Summary:** {json.dumps(dataset_summary, indent=2)}\n",
    "\n",
    "    **INSTRUCTIONS:**\n",
    "    1.  **Select a Covariate Balancing Method** from the following options:\n",
    "        - Propensity Score Matching (PSM)\n",
    "        - Inverse Probability of Treatment Weighting (IPTW)\n",
    "        - No Balancing\n",
    "\n",
    "    2.  **Select one or more Survival Analysis Methods** from the following options:\n",
    "        - Cox Proportional Hazards\n",
    "        - Kaplan-Meier Estimator\n",
    "        - Parametric Survival Models\n",
    "        - Random Survival Forest\n",
    "        - Doubly Robust Estimation\n",
    "\n",
    "    3.  **Provide a brief, clear rationale** for each choice, considering the data characteristics and research objectives.\n",
    "\n",
    "    **OUTPUT FORMAT:**\n",
    "    You MUST structure your response using the following XML-like tags. Do not include any other text outside these tags.\n",
    "    <BALANCING_METHOD>\n",
    "        <METHOD>Method Name</METHOD>\n",
    "        <RATIONALE>Your rationale here</RATIONALE>\n",
    "    </BALANCING_METHOD>\n",
    "    <SURVIVAL_ANALYSIS>\n",
    "        <METHODS>Method1, Method2</METHODS>\n",
    "        <RATIONALE>Your rationale here</RATIONALE>\n",
    "    </SURVIVAL_ANALYSIS>\n",
    "\n",
    "    **ANSWER:**\n",
    "    \"\"\"\n",
    "    state['answering_agents'].append(\"Statistician\")\n",
    "    state['prompts'].append(prompt)\n",
    "    response = llm.invoke(prompt, use_cache=False)\n",
    "    state['responses'].append(response)\n",
    "    \n",
    "    print(state['answering_agents'][-1])\n",
    "    print(state['responses'][-1])\n",
    "    print(\"------------------------------------------\")\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    # Use more robust regex parsing instead of splitting\n",
    "    try:\n",
    "        balancing_method = re.search(r\"<METHOD>(.*?)</METHOD>\", response, re.DOTALL).group(1).strip()\n",
    "        balancing_rationale = re.search(r\"<RATIONALE>(.*?)</RATIONALE>\", response, re.DOTALL).group(1).strip()\n",
    "        \n",
    "        # Find all rationale blocks and get the second one for survival\n",
    "        survival_rationale_match = re.findall(r\"<RATIONALE>(.*?)</RATIONALE>\", response, re.DOTALL)\n",
    "        survival_rationale = survival_rationale_match[1].strip() if len(survival_rationale_match) > 1 else \"\"\n",
    "\n",
    "        survival_methods_str = re.search(r\"<METHODS>(.*?)</METHODS>\", response, re.DOTALL).group(1).strip()\n",
    "        survival_methods = [method.strip() for method in survival_methods_str.split(\",\")]\n",
    "\n",
    "    except (AttributeError, IndexError) as e:\n",
    "        balancing_method = \"Propensity Score Matching (PSM)\"\n",
    "        balancing_rationale = f\"Defaulting to PSM due to parsing error: {e}\"\n",
    "        survival_methods = [\"Cox Proportional Hazards\"]\n",
    "        survival_rationale = f\"Defaulting to Cox model due to parsing error: {e}\"\n",
    "        \n",
    "    return balancing_method, balancing_rationale, survival_methods, survival_rationale\n",
    "\n",
    "\n",
    "def apply_covariate_balancing(state, method, df, categorical_covariates, continuous_covariates, ignore_covariates, list_of_treatment, list_of_outcome, outcome_definition, break_flag, results):\n",
    "    \"\"\"\n",
    "    Applies the selected covariate balancing method to the DataFrame.\n",
    "    Returns the balanced DataFrame and information about the balancing process.\n",
    "    \"\"\"\n",
    "    scaler = None\n",
    "    if method == \"Propensity Score Matching (PSM)\":\n",
    "        balanced_df, info, results, break_flag, scaler = perform_psm(df, categorical_covariates, continuous_covariates, ignore_covariates, list_of_treatment, list_of_outcome, outcome_definition, break_flag, results, state)\n",
    "    elif method == \"Inverse Probability of Treatment Weighting (IPTW)\":\n",
    "        balanced_df, info, results, break_flag = perform_iptw(df, categorical_covariates, continuous_covariates, ignore_covariates, list_of_treatment, list_of_outcome, outcome_definition, break_flag, results, state)\n",
    "    elif method == \"No Balancing\":\n",
    "        balanced_df, info = df.copy(), \"No balancing applied.\"\n",
    "    else:\n",
    "        balanced_df, info = df.copy(), \"No balancing applied.\"\n",
    "    \n",
    "    return balanced_df, info, break_flag, results, scaler\n",
    "\n",
    "\n",
    "def conduct_survival_analysis(state, methods, df, balanced_df, categorical_covariates, continuous_covariates, list_of_treatment, list_of_outcome, list_of_duration, outcome_definition, results, plots):\n",
    "    \"\"\"\n",
    "    Conducts survival analysis using various models.\n",
    "    Returns the results and generated plots.\n",
    "    \"\"\"\n",
    "    \n",
    "    str_info = \"Survival Analysis Summary:\"\n",
    "\n",
    "    # Semi-parametric Models (Cox Proportional Hazards)\n",
    "    if \"Cox Proportional Hazards\" in methods:\n",
    "        cox_summary, fig, cph_info = run_cox_model(balanced_df, list_of_treatment, list_of_outcome, list_of_duration, outcome_definition, categorical_covariates, continuous_covariates, state)\n",
    "        results['Cox Proportional Hazards Summary'] = cox_summary\n",
    "        results['Cox Proportional Hazards Info'] = cph_info\n",
    "        str_info = str_info + \"\\n\" + cph_info\n",
    "        plots['Cox Assumption Plots'] = fig\n",
    "    \n",
    "    # (Kaplan-Meier Estimator)\n",
    "    if \"Kaplan-Meier Estimator\" in methods:\n",
    "        km_results, km_plot = run_kaplan_meier(balanced_df, list_of_treatment, list_of_outcome, list_of_duration)\n",
    "        results['Kaplan-Meier Estimator'] = km_results\n",
    "        plots['Kaplan-Meier Plot'] = km_plot\n",
    "\n",
    "    # Parametric Models\n",
    "    if \"Parametric Survival Models\" in methods:\n",
    "        parametric_summary, parametric_info = run_parametric_model(balanced_df, list_of_treatment, list_of_outcome, list_of_duration, outcome_definition, state)\n",
    "        results['Parametric Survival Models Summary'] = parametric_summary\n",
    "        results['Parametric Survival Models Info'] = parametric_info\n",
    "        str_info = str_info + \"\\n\" + parametric_info\n",
    "\n",
    "    # Random Survival Forest\n",
    "    if \"Random Survival Forest\" in methods:\n",
    "        survival_prob_fig, survival_info = run_survival_forest(balanced_df, list_of_treatment, list_of_outcome, list_of_duration, categorical_covariates, continuous_covariates, outcome_definition, state)\n",
    "        plots['Random Survival Forest Plot'] = survival_prob_fig\n",
    "        str_info = str_info + \"\\n\" + survival_info\n",
    "\n",
    "    # Doubly Robust Estimation\n",
    "    if \"Doubly Robust Estimation\" in methods:\n",
    "        dr_results, dr_info = run_doubly_robust(df, list_of_treatment, list_of_outcome, categorical_covariates, continuous_covariates, outcome_definition, state)\n",
    "        results['Doubly Robust Estimation Results'] = dr_results\n",
    "        str_info = str_info + \"\\n\" + dr_info\n",
    "    \n",
    "    return results, plots, str_info\n",
    "\n",
    "\n",
    "def compute_SEM(dHRs):\n",
    "    '''Compute the standard error of the Monte Carlo mean'''\n",
    "    dHRs = np.array(dHRs)\n",
    "    SEM = np.mean([np.std(dHRs[:, i])/np.sqrt(dHRs.shape[0]) for i in range(dHRs.shape[1])])\n",
    "    return SEM\n",
    "\n",
    "\n",
    "def perform_ec_optmization(state, cache_folder='trials'):\n",
    "    cache_file = osp.join(cache_folder, f\"{state['variables']['trialid']}\", 'ec_optimization.pkl')\n",
    "    if os.path.exists(cache_file):\n",
    "        with open(cache_file, \"rb\") as f:\n",
    "            cached_results = pickle.load(f)\n",
    "            print(\"Loaded cached results from file.\")\n",
    "        return cached_results\n",
    "\n",
    "    all_dataframe = state['variables']['big_dataframe'] \n",
    "    ec_info = state['variables']['ec_patients_powerset_dataframe']\n",
    "\n",
    "    eligibility_list_rule_combos = ec_info['EligibilityCriteria'].tolist()\n",
    "    eligibility_list_rule_combos = [sorted(combo) for combo in eligibility_list_rule_combos]\n",
    "    patients_for_each_combo = ec_info['MatchedPatients'].tolist()\n",
    "\n",
    "    name_rules = np.unique([item for sublist in eligibility_list_rule_combos for item in sublist])\n",
    "    name_rules = np.sort(name_rules)\n",
    "\n",
    "    hrs_for_all_combos = []\n",
    "\n",
    "    # hr dictionary to save per patient_ids_for_combo so that calculations aren't repeated if the same patient_ids_for_combo are used\n",
    "    previous_patient_ids_for_combo = []\n",
    "\n",
    "    for i in range(len(eligibility_list_rule_combos)):\n",
    "        list_of_covariates = copy.deepcopy(state['variables']['Covariates'])\n",
    "        list_of_treatment = copy.deepcopy(state['variables']['Treatment'])\n",
    "        list_of_outcome = copy.deepcopy(state['variables']['Outcome'])\n",
    "        list_of_duration = copy.deepcopy(state['variables']['Duration'])\n",
    "        patient_column = state['variables']['patient_id']\n",
    "\n",
    "        # if list_of_treatment is a string, convert it to a list\n",
    "        if isinstance(list_of_treatment, str):\n",
    "            list_of_treatment = [list_of_treatment]\n",
    "        # if list_of_outcome is a string, convert it to a list\n",
    "        if isinstance(list_of_outcome, str):\n",
    "            list_of_outcome = [list_of_outcome]\n",
    "        # if list_of_duration is a string, convert it to a list\n",
    "        if isinstance(list_of_duration, str):\n",
    "            list_of_duration = [list_of_duration]\n",
    "\n",
    "        patient_ids_for_combo = patients_for_each_combo[i]\n",
    "\n",
    "        # Checking if HR has been calculated for the same patient_ids_for_combo\n",
    "        continue_flag = False\n",
    "        for j, previous_patient_ids in enumerate(previous_patient_ids_for_combo):\n",
    "            if set(previous_patient_ids) == set(patient_ids_for_combo):\n",
    "                hrs_for_all_combos.append(hrs_for_all_combos[j])\n",
    "                previous_patient_ids_for_combo.append(patient_ids_for_combo)\n",
    "                continue_flag = True\n",
    "                break\n",
    "        if continue_flag:\n",
    "            continue\n",
    "\n",
    "        dataframe = all_dataframe[all_dataframe[patient_column].isin(patient_ids_for_combo)]\n",
    "\n",
    "        df, categorical_covariates, continuous_covariates, list_of_treatment, list_of_outcome, list_of_duration = clean_data(list_of_covariates,\n",
    "                                                                                                                            list_of_treatment,\n",
    "                                                                                                                                list_of_outcome,\n",
    "                                                                                                                                list_of_duration,\n",
    "                                                                                                                                    dataframe)\n",
    "\n",
    "        break_flag = False\n",
    "        results = {}\n",
    "        plots = {}\n",
    "\n",
    "        balancing_method = \"Propensity Score Matching\"\n",
    "\n",
    "        # Step 3: Conduct Covariate Balancing\n",
    "        balanced_df, balancing_str_info, break_flag, results, scaler = apply_covariate_balancing(state, balancing_method, df, categorical_covariates, continuous_covariates, [], list_of_treatment, list_of_outcome, state['variables']['outcome_definition'], break_flag, results)\n",
    "        if scaler is not None:\n",
    "            # Inverse transform the balanced dataframe\n",
    "            balanced_df[continuous_covariates] = scaler.inverse_transform(balanced_df[continuous_covariates])\n",
    "\n",
    "        # Step 4: Conduct Survival Analysis\n",
    "        cph, data_cox = calculate_hazard_ratios(balanced_df, list_of_treatment, list_of_outcome, list_of_duration, categorical_covariates, continuous_covariates)\n",
    "        cox_summary = cph.summary.to_dict()\n",
    "        hr = cph.summary.loc[list_of_treatment[0]]['exp(coef)']\n",
    "        hrs_for_all_combos.append(hr)\n",
    "        previous_patient_ids_for_combo.append(patient_ids_for_combo)\n",
    "\n",
    "    # Get HR associated with no eligibility criteria\n",
    "    hr_empty = hrs_for_all_combos[eligibility_list_rule_combos.index([])]\n",
    "    # Get HR associated with all eligibility criteria, using the longest list of eligibility criteria\n",
    "    hr_full = hrs_for_all_combos[eligibility_list_rule_combos.index(max(eligibility_list_rule_combos, key=len))]\n",
    "    dHRs = []\n",
    "\n",
    "    n_rules = len(name_rules)\n",
    "    name_rules = np.array(name_rules)\n",
    "    tolerance = 0.001\n",
    "\n",
    "    iter_max = 1000\n",
    "    for m in range(iter_max):\n",
    "        dHR = np.zeros([n_rules])\n",
    "        idx = np.random.permutation(n_rules)\n",
    "        HRs = [hr_empty]\n",
    "        for i_rule in range(1, n_rules):\n",
    "            name_rules_subset = name_rules[idx[:i_rule]]\n",
    "            idx_subset = eligibility_list_rule_combos.index(sorted(list(name_rules_subset)))\n",
    "            HR = hrs_for_all_combos[idx_subset]\n",
    "            HRs.append(HR)\n",
    "        HRs.append(hr_full)\n",
    "        dHR[idx] = np.array([HRs[i]-HRs[i-1] for i in range(1, len(HRs))])\n",
    "        dHRs.append(dHR)\n",
    "        # Convergence checking\n",
    "        SEM = compute_SEM(dHRs)\n",
    "        print('Shapley Computation Iteration %d | SEM = %.4f' % (m, SEM))\n",
    "        if (m>0) and (SEM < tolerance):\n",
    "            print('Stopping criteria satisfied!')\n",
    "            break\n",
    "    if m == (iter_max-1):\n",
    "        print('Maximum iteration reached!')\n",
    "    shapley_value = np.mean(dHRs, axis=0)\n",
    "\n",
    "    with open(cache_file, \"wb\") as f:\n",
    "        pickle.dump((shapley_value, name_rules, hrs_for_all_combos), f)\n",
    "        print(f\"Results saved to {cache_file}.\")\n",
    "\n",
    "    return shapley_value, name_rules, hrs_for_all_combos\n",
    "\n",
    "\n",
    "def reask_subgroup_to_clinician(state, continuous_covariates_lower, categorical_covariates_lower, strat_covariate, list_of_outcome, balanced_df_subgroup):\n",
    "    subgroup_analyses = \"\"\n",
    "    for key, value in state['variables'][list_of_outcome[0]]['subgroup_analysis_dict'].items():\n",
    "        subgroup_analyses += f\"{key}\\n{value}\\n\\n\"\n",
    "\n",
    "    # Results are not significant so we want to ask the clinican to ask for a stratification on a covariate\n",
    "    # Provide all the continuous covariates and their ranges\n",
    "    # Provide all the categorical covariates and their unique values\n",
    "    continuous_covariates_records = {}\n",
    "    for col in continuous_covariates_lower:\n",
    "        continuous_covariates_records[col] = [balanced_df_subgroup[col].min(), balanced_df_subgroup[col].max()]\n",
    "    categorical_covariates_records = {}\n",
    "    for col in categorical_covariates_lower:\n",
    "        categorical_covariates_records[col] = balanced_df_subgroup[col].unique().tolist()\n",
    "    issue = f'''\n",
    "    Clinician, the current hazard ratio is not significant for any subgroup analyses. Please provide a covariate to stratify on. Also provide the level to stratify on.\n",
    "\n",
    "    You are given the following covariates and their ranges:\n",
    "\n",
    "    Continuous Covariates:\n",
    "    {continuous_covariates_records}\n",
    "\n",
    "    Categorical Covariates:\n",
    "    {categorical_covariates_records}\n",
    "\n",
    "    You are also given previous stratification results:\n",
    "    {subgroup_analyses}\n",
    "\n",
    "    Do **NOT** stratify on this covariate: {strat_covariate}\n",
    "    \n",
    "    Expected Structure Output:\n",
    "    STRATIFY ON: <Covariate>\n",
    "    LEVEL: <Level>\n",
    "\n",
    "    Provide a **DIFFERENT** covariate to stratify on. It should not be in the previous stratification results or {strat_covariate}.\n",
    "    The Covariate should be one of the covariates provided above. It should be spelled and use the exact same capitalization as in the list.\n",
    "    Level should be a value within the range of the covariate. \\\n",
    "    Level **MUST** be a number. Do not output a placeholder or a string. It will be parsed and used for stratification.\n",
    "\n",
    "    Do not provide additional information or context in the response. No explanation is needed. Provide BOTH \"STRATIFY ON\" and \"LEVEL\" in the response.\n",
    "    ONLY provide **ONE** stratification.\n",
    "    '''\n",
    "    state['recent_response'] = issue\n",
    "    state['sending_agent'] = \"Statistician\"\n",
    "    state['predefined_next_agent'] = \"Clinician\"\n",
    "    state['predefined_next_question'] = issue\n",
    "    state['next'] = \"__end__\"\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "def _handle_subgroup_analysis(state, list_of_outcome, list_of_duration):\n",
    "    \"\"\"\n",
    "    Handles the subgroup analysis based on the ongoing question and the current state of the variables.\n",
    "    state: The current state of the system.\n",
    "    \"\"\"\n",
    "    list_of_outcomes = state['variables']['Outcome']\n",
    "    list_of_outcomes = [outcome.lower() for outcome in list_of_outcomes]\n",
    "\n",
    "    if 'number_time_entered_subgroup_analysis' not in state['variables'][list_of_outcome[0]]:\n",
    "        state['variables'][list_of_outcome[0]]['number_time_entered_subgroup_analysis'] = 0\n",
    "\n",
    "    state['variables'][list_of_outcome[0]]['number_time_entered_subgroup_analysis'] += 1\n",
    "\n",
    "    subgroup_results_str = \"\"\n",
    "    balanced_df_subgroup = state['variables'][list_of_outcome[0]]['balanced_df'].copy()\n",
    "    continuous_covariates = state['variables'][list_of_outcome[0]]['continuous_covariates']\n",
    "    categorical_covariates = state['variables'][list_of_outcome[0]]['categorical_covariates']\n",
    "    list_of_covariates = state['variables'][list_of_outcome[0]]['categorical_covariates'] + state['variables'][list_of_outcome[0]]['continuous_covariates']\n",
    "    list_of_treatment = state['variables']['Treatment']\n",
    "    response = state['ongoing_question']\n",
    "\n",
    "    if state['variables'][list_of_outcome[0]]['number_time_entered_subgroup_analysis'] <= 5:\n",
    "        # --------------------------\n",
    "        # Step 1: Load and Prepare Data\n",
    "        # --------------------------\n",
    "        ## 1.1 stratified variables\n",
    "        # get the last substring with \"STRATIFY ON:\"\n",
    "        response = \"STRATIFY ON:\" + response.split(\"STRATIFY ON:\")[-1]\n",
    "\n",
    "        strat_covariate = re.split(r'[\\n,]', response.split(\"STRATIFY ON:\")[1])[0].strip()\n",
    "\n",
    "        # if list_of_treatment is a string, convert it to a list\n",
    "        if isinstance(list_of_treatment, str):\n",
    "            list_of_treatment = [list_of_treatment]\n",
    "        # if list_of_outcome is a string, convert it to a list\n",
    "        if isinstance(list_of_outcome, str):\n",
    "            list_of_outcome = [list_of_outcome]\n",
    "        # if list_of_duration is a string, convert it to a list\n",
    "        if isinstance(list_of_duration, str):\n",
    "            list_of_duration = [list_of_duration]\n",
    "        list_of_covariates_lower = [col.lower() for col in list_of_covariates]\n",
    "        continuous_covariates_lower = [col.lower() for col in continuous_covariates]\n",
    "        categorical_covariates_lower = [col.lower() for col in categorical_covariates]\n",
    "        \n",
    "        # Make all columns in balanced_df_subgroup which are in list_of_covariates_lower lowercase\n",
    "        new_cols = []\n",
    "        for col in balanced_df_subgroup.columns:\n",
    "            if (col.lower() in continuous_covariates_lower) or (col.lower() in categorical_covariates_lower):\n",
    "                new_cols.append(col.lower())\n",
    "            else:\n",
    "                new_cols.append(col)\n",
    "        balanced_df_subgroup.columns = new_cols\n",
    "\n",
    "        ## 1.2 stratified values(levels)\n",
    "        # level_strat = response.split(\"LEVEL:\")[1].split(\"\\n|,\")[0].strip()\n",
    "        level_strat = re.split(r'[\\n,]', response.split(\"LEVEL:\")[1])[0].strip()\n",
    "        # if >, <, >=, <= is in the level_strat, then convert it to the correct format\n",
    "        if \">=\" in level_strat:\n",
    "            level_strat = level_strat.replace(\">=\", \"\").strip()\n",
    "        elif \"<=\" in level_strat:\n",
    "            level_strat = level_strat.replace(\"<=\", \"\").strip()\n",
    "        elif \">\" in level_strat:\n",
    "            level_strat = level_strat.replace(\">\", \"\").strip()\n",
    "        elif \"<\" in level_strat:\n",
    "            level_strat = level_strat.replace(\"<\", \"\").strip()\n",
    "\n",
    "        if '`' in level_strat:\n",
    "            level_strat = level_strat.replace(\"`\", \"\").strip()\n",
    "        if '%' in level_strat:\n",
    "            level_strat = level_strat.replace(\"%\", \"\").strip()\n",
    "\n",
    "        \n",
    "        # Check if level_strat can be converted to a float\n",
    "        try:\n",
    "            level_strat = float(level_strat.strip())\n",
    "        except:\n",
    "            return reask_subgroup_to_clinician(state, continuous_covariates_lower, categorical_covariates_lower, strat_covariate, list_of_outcome, balanced_df_subgroup)\n",
    "\n",
    "\n",
    "        # make strat_covariate lowercase\n",
    "        strat_covariate = strat_covariate.lower()\n",
    "\n",
    "        # ask llm to make sure strat_covariate is in all_covariates_lower and if not find the closest match\n",
    "        all_covariates_lower = continuous_covariates_lower + categorical_covariates_lower\n",
    "        if strat_covariate not in all_covariates_lower:\n",
    "            llm_prompt = f\"\"\"\n",
    "            You are a statistician. The covariate to stratify on is not in the list of covariates. Please provide the closest match to the covariate to stratify on.\n",
    "\n",
    "            The covariate to stratify on is: {strat_covariate}\n",
    "\n",
    "            The available covariates are: {all_covariates_lower}\n",
    "\n",
    "            The match **MUST** be one of the covariates provided above. It **SHOULD** be spelled the **EXACT** same way.\n",
    "\n",
    "            Do not provide additional information or context in the response. No explanation is needed.\n",
    "\n",
    "            Expected Structure Output:\n",
    "            STRATIFY ON: <Covariate>\n",
    "            \"\"\"\n",
    "            \n",
    "            state['answering_agents'].append(\"Statistician\")\n",
    "            state['prompts'].append(llm_prompt)\n",
    "            response_llm = llm.invoke(llm_prompt, use_cache=False)\n",
    "            state['responses'].append(response_llm)\n",
    "\n",
    "            print(state['answering_agents'][-1])\n",
    "            print(state['responses'][-1])\n",
    "            print(\"------------------------------------------\")\n",
    "            print(\"\\n\\n\")\n",
    "\n",
    "            if \"STRATIFY ON\" in response_llm:\n",
    "                strat_covariate = response_llm.split(\"STRATIFY ON:\")[1].split(\"\\n|,\")[0].strip().lower()\n",
    "\n",
    "\n",
    "        key = f\"Stratifying by {strat_covariate} at {level_strat}, Subgroup A: {strat_covariate} < {level_strat}, Subgroup B: {strat_covariate} >= {level_strat}\"\n",
    "\n",
    "        # Check if strat_covariate is in balanced_df_subgroup as a final check\n",
    "        if strat_covariate not in balanced_df_subgroup.columns:\n",
    "            return reask_subgroup_to_clinician(state, continuous_covariates_lower, categorical_covariates_lower, strat_covariate, list_of_outcome, balanced_df_subgroup)\n",
    "\n",
    "\n",
    "        # check if level_strat is within the range of the strat_covariate\n",
    "        if balanced_df_subgroup[strat_covariate].min() > level_strat or balanced_df_subgroup[strat_covariate].max() < level_strat:\n",
    "            return reask_subgroup_to_clinician(state, continuous_covariates_lower, categorical_covariates_lower, strat_covariate, list_of_outcome, balanced_df_subgroup)\n",
    "\n",
    "                    \n",
    "        balanced_df_subgroup['strat_group'] = np.where(balanced_df_subgroup[strat_covariate] < level_strat, 'SubgroupA', 'SubgroupB')\n",
    "        balanced_df_subgroup[f\"{list_of_treatment[0]}_{strat_covariate}\"] = balanced_df_subgroup[list_of_treatment[0]] * balanced_df_subgroup['strat_group'].map({'SubgroupA': 0, 'SubgroupB': 1})\n",
    "\n",
    "        # --------------------------\n",
    "        # Step 2: Fit Cox PH Model\n",
    "        # --------------------------\n",
    "        cph = lifelines.fitters.coxph_fitter.CoxPHFitter(penalizer=0.1)\n",
    "        cph_formula = f\"{list_of_treatment[0]} + strat_group + {list_of_treatment[0]}_{strat_covariate}\"\n",
    "        # Add all the other covariates to the formula\n",
    "        for covariate in list_of_covariates_lower:\n",
    "            if covariate not in [list_of_treatment[0], strat_covariate]:\n",
    "                cph_formula += f\" + {covariate}\"\n",
    "\n",
    "        try:\n",
    "            if \"iptw_weight\" in list(balanced_df_subgroup):\n",
    "                cph.fit(\n",
    "                    balanced_df_subgroup,\n",
    "                    duration_col=list_of_duration[0],\n",
    "                    event_col=list_of_outcome[0],\n",
    "                    robust=True,\n",
    "                    weights_col=\"iptw_weight\",\n",
    "                    formula=cph_formula)\n",
    "\n",
    "            else:\n",
    "                cph.fit(\n",
    "                    balanced_df_subgroup,\n",
    "                    duration_col=list_of_duration[0],\n",
    "                    event_col=list_of_outcome[0],\n",
    "                    robust=True,\n",
    "                    formula=cph_formula\n",
    "                )\n",
    "        except:\n",
    "            return reask_subgroup_to_clinician(state, continuous_covariates_lower, categorical_covariates_lower, strat_covariate, list_of_outcome, balanced_df_subgroup)\n",
    "\n",
    "\n",
    "        # --------------------------\n",
    "        # Step 3: Extract Coefficients and Covariance Matrix\n",
    "        # --------------------------\n",
    "        params = cph.params_\n",
    "        cov_matrix = cph.variance_matrix_\n",
    "\n",
    "        # --------------------------\n",
    "        # Step 4: Calculate HRs\n",
    "        # --------------------------\n",
    "        beta_treatment = params[list_of_treatment[0]]\n",
    "        beta_interaction = params[f\"{list_of_treatment[0]}_{strat_covariate}\"]\n",
    "        hr_subgroupA = np.exp(beta_treatment)\n",
    "        hr_subgroupB = np.exp(beta_treatment + beta_interaction)\n",
    "\n",
    "        # --------------------------\n",
    "        # Step 5: Calculate CIs\n",
    "        # --------------------------\n",
    "        se_treatment = cph.standard_errors_[list_of_treatment[0]]\n",
    "        se_interaction = cph.standard_errors_[f\"{list_of_treatment[0]}_{strat_covariate}\"]\n",
    "        cov_treatment_interaction = cov_matrix.loc[list_of_treatment[0], f\"{list_of_treatment[0]}_{strat_covariate}\"]\n",
    "        se_subgroupB = np.sqrt(cov_matrix.loc[list_of_treatment[0], list_of_treatment[0]] +\n",
    "                            cov_matrix.loc[f\"{list_of_treatment[0]}_{strat_covariate}\", f\"{list_of_treatment[0]}_{strat_covariate}\"] +\n",
    "                            2 * cov_matrix.loc[list_of_treatment[0], f\"{list_of_treatment[0]}_{strat_covariate}\"])\n",
    "        ci_subgroupA_lower = np.exp(beta_treatment - 1.96 * se_treatment)\n",
    "        ci_subgroupA_upper = np.exp(beta_treatment + 1.96 * se_treatment)\n",
    "        ci_subgroupB_lower = np.exp(beta_treatment + beta_interaction - 1.96 * se_subgroupB)\n",
    "        ci_subgroupB_upper = np.exp(beta_treatment + beta_interaction + 1.96 * se_subgroupB)\n",
    "\n",
    "        # --------------------------\n",
    "        # Step 6: Calculate p-values\n",
    "        # --------------------------\n",
    "        p_subgroupA = cph.summary.loc[list_of_treatment[0], 'p']\n",
    "        z_subgroupB = (beta_treatment + beta_interaction) / se_subgroupB\n",
    "        p_subgroupB = 2 * (1 - scipy.stats.norm.cdf(abs(z_subgroupB)))\n",
    "\n",
    "        any_significance = False\n",
    "\n",
    "        subgroup_results_str += f\"HR for Treatment vs. Control for Subgroup A: {hr_subgroupA:.4f}\\n\"\n",
    "        subgroup_results_str += f\"95% CI for HR in Subgroup A: ({ci_subgroupA_lower:.4f}, {ci_subgroupA_upper:.4f})\\n\"\n",
    "        subgroup_results_str += f\"p-value for HR in Subgroup A: {p_subgroupA:.4f}\\n\"\n",
    "\n",
    "        if p_subgroupA < 0.05:\n",
    "            any_significance = True\n",
    "            subgroup_results_str += f\"Subgroup A is statistically significant.\\n\\n\"\n",
    "        else:\n",
    "            subgroup_results_str += f\"Subgroup A is NOT statistically significant.\\n\\n\"\n",
    "\n",
    "        subgroup_results_str += f\"HR for Treatment vs. Control for Subgroup B: {hr_subgroupB:.4f}\\n\"\n",
    "        subgroup_results_str += f\"95% CI for HR in Subgroup B: ({ci_subgroupB_lower:.4f}, {ci_subgroupB_upper:.4f})\\n\"\n",
    "        subgroup_results_str += f\"p-value for HR in Subgroup B: {p_subgroupB:.4f}\\n\"\n",
    "\n",
    "        if p_subgroupB < 0.05:\n",
    "            any_significance = True\n",
    "            subgroup_results_str += f\"Subgroup B is statistically significant.\\n\\n\"\n",
    "        else:\n",
    "            subgroup_results_str += f\"Subgroup B is NOT statistically significant.\\n\\n\"\n",
    "\n",
    "        state['variables'][list_of_outcome[0]]['subgroup_analysis_dict'][key] = subgroup_results_str\n",
    "\n",
    "        if not any_significance and len(state['variables'][list_of_outcome[0]]['subgroup_analysis_dict']) < 5:\n",
    "            return reask_subgroup_to_clinician(state, continuous_covariates_lower, categorical_covariates_lower, strat_covariate, list_of_outcome, balanced_df_subgroup)\n",
    "\n",
    "    state['ongoing_question'] = \"\"\n",
    "    state['current_information'] = f\"Finished analysis for {list_of_outcome[0]}.\"\n",
    "    state['variables']['completed_outcomes'].append(list_of_outcome[0])\n",
    "    if len(list_of_outcomes) > (state['variables']['current_outcome_index'] + 1):\n",
    "        state['ongoing_question'] = f\"Please proceed to the next outcome: {list_of_outcomes[state['variables']['current_outcome_index'] + 1]}.\"\n",
    "        state['current_information'] += f\" Proceeding to the next outcome: {list_of_outcomes[state['variables']['current_outcome_index'] + 1]}.\"\n",
    "    state['variables']['current_outcome_index'] += 1\n",
    "    state['next'] = \"Statistician\"\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "def _parse_covariate_modifications(state, dataframe, list_of_outcome, llm):\n",
    "    most_recent_response = state['ongoing_question'] \n",
    "    llm_prompt = f\"\"\"\n",
    "    You are a statistician. Based on the most recent response, determine if there are any covariates to drop or add.\n",
    "    You might see a response like the following:\n",
    "\n",
    "    body mass index (BMI)|DROP|N/A|Less critical for survival analysis in sepsis patients compared to other factors.\n",
    "    creatinine|SUBSTITUTE|blood urea nitrogen (BUN)|BUN can reflect kidney function similarly to creatinine.\n",
    "    heart rate|SUBSTITUTE|mean arterial pressure (MAP)|MAP indirectly reflects cardiovascular function and is more consistently available.\n",
    "\n",
    "    In this case, the covariates to drop are 'body mass index (BMI)', 'creatinine', and 'heart rate'\n",
    "    The covariates to add are 'blood urea nitrogen (BUN)' and 'mean arterial pressure (MAP)'\n",
    "\n",
    "    Sometimes, you might decide to not drop or add covariates, but rather just ignore it. This might be due to \\\n",
    "    the fact that the the covariate cannot be substituted or dropped or post-processing after balancing is the recommended approach.\n",
    "    In that case, add this to ignore covariates.\n",
    "\n",
    "    Sometimes the most recent response does not have to do anything with covariates, \\\n",
    "    in that case, output 'No covariates to drop' or 'No covariates to add'.\n",
    "\n",
    "    The most recent response is as follows:\n",
    "    {most_recent_response}\n",
    "\n",
    "    If there are any covariates to drop, provide a list of the covariates to be dropped in the expected output structure:\n",
    "    DROP COVARIATES: <Covariate1>,<Covariate2>, ...\n",
    "    ADD COVARIATES: <Covariate1>,<Covariate2>, ...\n",
    "    IGNORE COVARIATES: <Covariate1>,<Covariate2>, ...\n",
    "\n",
    "    If no covariates are to be dropped, output 'No covariates to drop'. Do not output 'DROP COVARIATES: '.\n",
    "    If no covariates are to be added, output 'No covariates to add'. Do not output 'ADD COVARIATES: '.\n",
    "\n",
    "    Do not additional information or context in the response. No explanations or extra formatting that what is described above for the output structure.\n",
    "\n",
    "    ANSWER:\n",
    "    \"\"\"\n",
    "\n",
    "    state['answering_agents'].append(\"Statistician\")\n",
    "    state['prompts'].append(llm_prompt)\n",
    "    response_llm_drop = llm.invoke(llm_prompt, use_cache=False)\n",
    "    state['responses'].append(response_llm_drop)\n",
    "    \n",
    "    print(state['answering_agents'][-1])\n",
    "    print(state['responses'][-1])\n",
    "    print(\"------------------------------------------\")\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    covariates_to_drop = []\n",
    "    covariates_to_add = []\n",
    "    covariates_to_ignore = []\n",
    "    try:\n",
    "        if \"No covariates to drop\" in response_llm_drop:\n",
    "            covariates_to_drop = []\n",
    "        if \"No covariates to add\" in response_llm_drop:\n",
    "            covariates_to_add = []\n",
    "        if \"DROP COVARIATES\" in response_llm_drop:\n",
    "            covariates_to_drop = response_llm_drop.split(\"DROP COVARIATES:\")[1].strip().split(\",\")\n",
    "            # strip each covariate and convert to lower case\n",
    "            covariates_to_drop = [covariate.strip().lower() for covariate in covariates_to_drop]\n",
    "        if \"ADD COVARIATES\" in response_llm_drop:\n",
    "            covariates_to_add = response_llm_drop.split(\"ADD COVARIATES:\")[1].strip().split(\",\")\n",
    "            # strip each covariate and convert to lower case\n",
    "            covariates_to_add = [covariate.strip().lower() for covariate in covariates_to_add]\n",
    "        if \"IGNORE COVARIATES\" in response_llm_drop:\n",
    "            covariates_to_ignore = response_llm_drop.split(\"IGNORE COVARIATES:\")[1].strip().split(\",\")\n",
    "            # strip each covariate and convert to lower case\n",
    "            covariates_to_ignore = [covariate.strip().lower() for covariate in covariates_to_ignore]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # See if there are columns with only NaN values\n",
    "    columns_with_nan = dataframe.columns[dataframe.isna().all()].tolist()\n",
    "    # Add these columns to the drop covariates\n",
    "    covariates_to_drop.extend(columns_with_nan)\n",
    "\n",
    "    state['variables'][list_of_outcome[0]]['ignore_covariates'].extend(covariates_to_ignore)\n",
    "    state['variables'][list_of_outcome[0]]['add_covariates'].extend(covariates_to_add)\n",
    "    state['variables'][list_of_outcome[0]]['drop_covariates'].extend(covariates_to_drop)\n",
    "\n",
    "    # Remove any covariates in covariates_to_add that are in covariates_to_drop\n",
    "    state['variables'][list_of_outcome[0]]['add_covariates'] = [covariate for covariate in state['variables'][list_of_outcome[0]]['add_covariates'] if covariate not in state['variables'][list_of_outcome[0]]['drop_covariates']]\n",
    "\n",
    "    # make sure there are no duplicates in ignore_covariates, add_covariates, and drop_covariates\n",
    "    state['variables'][list_of_outcome[0]]['ignore_covariates'] = list(set(state['variables'][list_of_outcome[0]]['ignore_covariates']))\n",
    "    state['variables'][list_of_outcome[0]]['add_covariates'] = list(set(state['variables'][list_of_outcome[0]]['add_covariates']))\n",
    "    state['variables'][list_of_outcome[0]]['drop_covariates'] = list(set(state['variables'][list_of_outcome[0]]['drop_covariates']))\n",
    "\n",
    "    covariates_to_add = state['variables'][list_of_outcome[0]]['add_covariates']\n",
    "    covariates_to_drop = state['variables'][list_of_outcome[0]]['drop_covariates']\n",
    "\n",
    "    return state, covariates_to_add, covariates_to_drop, covariates_to_ignore\n",
    "\n",
    "\n",
    "def _generate_final_report(state: dict, llm: LLMZoo) -> dict:\n",
    "    \"\"\"\n",
    "    Assembles all analysis results, calculates sample size, and uses an LLM\n",
    "    to generate a final, structured report.\n",
    "    \"\"\"\n",
    "    # --- 1. GATHER AND STRUCTURE RESULTS ---\n",
    "    outcomes = [o.lower() for o in state['variables'].get('Outcome', [])]\n",
    "    outcome_defs = state['variables'].get('outcome_definitions', [])\n",
    "    \n",
    "    # Primary Outcome\n",
    "    primary_outcome_col = outcomes[0]\n",
    "    primary_outcome_def = outcome_defs[0]\n",
    "    primary_results_data = state['variables'].get(primary_outcome_col, {})\n",
    "    primary_analysis_str = primary_results_data.get('subgroup_analysis_dict', {}).get('No Stratification', 'Not performed.')\n",
    "\n",
    "    # Trial Protocol\n",
    "    trial_protocol = state['variables'][primary_outcome_col].get('trial_protocol', 'No trial protocol provided.')\n",
    "\n",
    "    # Balancing Information for Primary Outcome\n",
    "    balancing_info = state['variables'][primary_outcome_col].get('balancing_information', 'No balancing information provided.')\n",
    "    \n",
    "    # Primary Subgroup Analyses\n",
    "    primary_subgroup_list = []\n",
    "    for key, value in primary_results_data.get('subgroup_analysis_dict', {}).items():\n",
    "        if key != 'No Stratification':\n",
    "            primary_subgroup_list.append(value)\n",
    "    primary_subgroup_str = \"\\n\\n\".join(primary_subgroup_list) if primary_subgroup_list else \"No significant subgroups were found or explored.\"\n",
    "\n",
    "    # Secondary/Adverse Event Outcomes\n",
    "    secondary_results_list = []\n",
    "    if len(outcomes) > 1:\n",
    "        for i, outcome_col in enumerate(outcomes[1:]):\n",
    "            outcome_data = state['variables'].get(outcome_col, {})\n",
    "            analysis_summary = outcome_data.get('subgroup_analysis_dict', {}).get('No Stratification', 'Not performed.')\n",
    "            secondary_results_list.append(f\"--- Outcome: {outcome_defs[i+1]} ---\\n{analysis_summary}\")\n",
    "    secondary_results_str = \"\\n\\n\".join(secondary_results_list) if secondary_results_list else \"No secondary outcomes were analyzed.\"\n",
    "\n",
    "    # --- 2. PERFORM SAMPLE SIZE CALCULATION ---\n",
    "    sample_size_str = \"Sample size calculation was not performed.\"\n",
    "    primary_cox_summary = primary_results_data.get('results', {}).get('Cox Proportional Hazards Summary')\n",
    "    if primary_cox_summary is not None:\n",
    "        try:\n",
    "            treatment_col = state['variables']['Treatment']\n",
    "            if isinstance(treatment_col, list): treatment_col = treatment_col[0]\n",
    "            \n",
    "            hr = primary_cox_summary['exp(coef)'][treatment_col]\n",
    "            size, _ = compute_cox_sample_size(\n",
    "                primary_results_data['balanced_df'],\n",
    "                treatment_col,\n",
    "                primary_outcome_col,\n",
    "                state['variables']['Duration'][0].lower(),\n",
    "                hr\n",
    "            )\n",
    "            sample_size_str = f\"Based on the observed data and effect size, the estimated required sample size to achieve 80% power at an alpha of 0.05 is {size} patients.\"\n",
    "        except Exception as e:\n",
    "            sample_size_str = f\"Could not perform sample size calculation due to an error: {e}\"\n",
    "\n",
    "    # -- E-Value Calculation --\n",
    "    evalue_str = \"E-Value calculation was not performed.\"\n",
    "    primary_cox_summary = primary_results_data.get('results', {}).get('Cox Proportional Hazards Summary')\n",
    "    if primary_cox_summary is not None:\n",
    "        try:\n",
    "            hr = primary_cox_summary['exp(coef)'][treatment_col]\n",
    "            evalue_results = calculate_evalues(\n",
    "                primary_results_data['balanced_df'],\n",
    "                primary_outcome_col,\n",
    "                hr\n",
    "            )\n",
    "            evalue = evalue_results[\"e_values\"][\"point_estimate\"]\n",
    "            evalue_str = f\"The minimum strength of association that an unmeasured confounder would need to have with both the exposure and the outcome to completey eliminate the observed effect is {evalue}. \"\n",
    "        except Exception as e:\n",
    "            evalue_str = f\"Could not perform E-Value calculation due to an error.\"\n",
    "\n",
    "    \n",
    "    # --- 3. CONSTRUCT THE IMPROVED LLM PROMPT ---\n",
    "    report_context = f\"\"\"\n",
    "    ### INITIAL QUESTION\n",
    "    {state[\"initial_question\"]}\n",
    "\n",
    "    ### PRIMARY OUTCOME: {primary_outcome_def}\n",
    "    #### Trial Protocol\n",
    "    {trial_protocol}\n",
    "\n",
    "    #### Balancing Information\n",
    "    {balancing_info}\n",
    "\n",
    "    #### Main Analysis Results\n",
    "    {primary_analysis_str}\n",
    "\n",
    "    #### Subgroup Analysis Results\n",
    "    {primary_subgroup_str}\n",
    "\n",
    "    ### SECONDARY / ADVERSE EVENT OUTCOMES\n",
    "    {secondary_results_str}\n",
    "\n",
    "    ### SAMPLE SIZE CALCULATION\n",
    "    {sample_size_str}\n",
    "\n",
    "    ### E-VALUE CALCULATION\n",
    "    {evalue_str}\n",
    "    \"\"\"\n",
    "\n",
    "    final_report_prompt = f\"\"\"\n",
    "    You are an expert medical writer and statistician tasked with composing a formal, publication-quality report based on the results of a trial emulation study.\n",
    "\n",
    "    **CONTEXT AND DATA:**\n",
    "    You are provided with the initial research question and a structured summary of all statistical analyses performed.\n",
    "    ---\n",
    "    {report_context}\n",
    "    ---\n",
    "\n",
    "    **INSTRUCTIONS:**\n",
    "    Based ONLY on the provided context and data, generate a complete and well-structured report. The report MUST include all of the following sections, in this order:\n",
    "\n",
    "    1.  **Title Page:** A clear and informative title for the study.\n",
    "    2.  **Abstract:** A concise summary covering the study's objective, methods, key results (for primary and secondary outcomes), and conclusions.\n",
    "    3.  **Introduction:** Provide background on the clinical question, the rationale for the trial emulation, and the study's objectives.\n",
    "    4.  **Methods:** Briefly describe the data sources, key eligibility criteria, treatment strategies, and the statistical methods used (e.g., confounder adjustment, outcome modeling). Also mention biases that were addressed.\n",
    "    5.  **Results:** Present the main findings.\n",
    "        * Start with the primary outcome results.\n",
    "        * Incorporate key numerical results, such as Hazard Ratios (HR), 95% Confidence Intervals (CI), and p-values where provided.\n",
    "        * Include the results from any subgroup analyses that were performed.\n",
    "        * Detail the findings for all secondary/adverse event outcomes.\n",
    "    6.  **Discussion:** Interpret the key findings. Discuss the consistency of the results, the study's strengths, and its limitations (e.g., if the sample size was noted as a concern). Mention implications for future research.\n",
    "    7.  **Conclusion:** Briefly summarize the study's main takeaways and contributions.\n",
    "    8.  **References:** Add a placeholder section for references.\n",
    "\n",
    "    **WRITING STYLE:**\n",
    "    * Be objective, formal, and use clear scientific language.\n",
    "    * When reporting numerical results, explain their meaning in the context of the study (e.g., \"The hazard ratio of 1.10 suggests a 10% increased risk...\").\n",
    "    * Pay close attention to confidence intervals and p-values when making statements about statistical significance.\n",
    "\n",
    "    Generate the complete report. Provide ONLY the report and NOTHING else.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 4. CALL LLM AND UPDATE STATE ---\n",
    "    final_report = llm.invoke(final_report_prompt, use_cache=False)\n",
    "    \n",
    "    # This logging was in the original code, it's good practice to keep it\n",
    "    state['answering_agents'].append(\"Statistician\")\n",
    "    state['prompts'].append(final_report_prompt)\n",
    "    state['responses'].append(final_report)\n",
    "    \n",
    "    print(\"--- Agent: Statistician ---\")\n",
    "    print(final_report)\n",
    "    print(\"------------------------------------------\\n\\n\")\n",
    "\n",
    "    report = final_report.split(\"<|eot_id|>\")[0].strip()\n",
    "    state['current_report'] = report\n",
    "    state['recent_response'] = report + \"\\n\\nPlease review the report and provide feedback.\"\n",
    "    state['sending_agent'] = \"Statistician\"\n",
    "    state['predefined_next_agent'] = \"Reviewer\"\n",
    "    state['next'] = \"__end__\"\n",
    "    \n",
    "    print(\"Generated Report by Statistician\")\n",
    "\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the state class for the agents\n",
    "class TopAgentState(MessagesState):\n",
    "    next: Literal[\"MainSupervisor\", \"ClinicianAgentInvoker\", \"InformaticianAgentInvoker\", \"TrialistInvoker\", \"__end__\"]\n",
    "    initial_question: str = \"\"  # Initial user question\n",
    "    predefined_next_agent: str = \"\"  # Predefined next agent\n",
    "    predefined_next_question: str = \"\"  # Predefined next question\n",
    "    sending_agent: str = \"\"  # Sending agent, agent who sent the most recent response\n",
    "    ongoing_question: str = \"\"  # Ongoing question\n",
    "    recent_response: str = \"\"  # Most recent response from agent\n",
    "    current_information: str = \"\"  # All collected information, minimized for entry into the next agent\n",
    "    all_information: str = \"\"  # All collected information\n",
    "    all_information_index: None  # Index for the faiss index\n",
    "    all_information_database: None  # Database for the faiss index\n",
    "    current_report: str = \"\"  # Current report\n",
    "    final_summary: str = \"\"  # Final summary\n",
    "    prompts: List[str] = []\n",
    "    responses: List[str] = []\n",
    "    answering_agents: List[str] = []\n",
    "    # Statistician specific variables\n",
    "    variables: Dict[str, Any] = {}\n",
    "\n",
    "class InformaticianState(MessagesState):\n",
    "    next: Literal[\"Informatician\", \"__end__\"]\n",
    "    initial_question: str = \"\"  # Initial user question\n",
    "    predefined_next_agent: str = \"\"  # Predefined next agent\n",
    "    predefined_next_question: str = \"\"  # Predefined next question\n",
    "    sending_agent: str = \"\"  # Sending agent, agent who sent the most recent response\n",
    "    ongoing_question: str = \"\"  # Ongoing question\n",
    "    recent_response: str = \"\"  # Most recent response from agent\n",
    "    current_information: str = \"\"  # All collected information, minimized for entry into the next agent\n",
    "    all_information: str = \"\"  # All collected information\n",
    "    all_information_index: None  # Index for the faiss index\n",
    "    all_information_database: None  # Database for the faiss index\n",
    "    current_report: str = \"\"  # Current report\n",
    "    final_summary: str = \"\"  # Final summary\n",
    "    prompts: List[str] = []\n",
    "    responses: List[str] = []\n",
    "    answering_agents: List[str] = []\n",
    "    # Informatician specific variables\n",
    "    variables: Dict[str, Any] = {}\n",
    "\n",
    "class StatisticianState(MessagesState):\n",
    "    next: Literal[\"Statistician\", \"__end__\"]\n",
    "    initial_question: str = \"\"  # Initial user question\n",
    "    predefined_next_agent: str = \"\"  # Predefined next agent\n",
    "    predefined_next_question: str = \"\"  # Predefined next question\n",
    "    sending_agent: str = \"\"  # Sending agent, agent who sent the most recent response\n",
    "    ongoing_question: str = \"\"  # Ongoing question\n",
    "    recent_response: str = \"\"  # Most recent response from agent\n",
    "    current_information: str = \"\"  # All collected information, minimized for entry into the next agent\n",
    "    all_information: str = \"\"  # All collected information\n",
    "    all_information_index: None  # Index for the faiss index\n",
    "    all_information_database: None  # Database for the faiss index\n",
    "    current_report: str = \"\"  # Current report\n",
    "    final_summary: str = \"\"  # Final summary\n",
    "    prompts: List[str] = []\n",
    "    responses: List[str] = []\n",
    "    answering_agents: List[str] = []\n",
    "    # Statistician specific variables\n",
    "    variables: Dict[str, Any] = {}\n",
    "\n",
    "class ClinicianState(MessagesState):\n",
    "    next: Literal[\"Clinician\" \"__end__\"]\n",
    "    initial_question: str = \"\"  # Initial user question\n",
    "    predefined_next_agent: str = \"\"  # Predefined next agent\n",
    "    predefined_next_question: str = \"\"  # Predefined next question\n",
    "    sending_agent: str = \"\"  # Sending agent, agent who sent the most recent response\n",
    "    ongoing_question: str = \"\"  # Ongoing question\n",
    "    recent_response: str = \"\"  # Most recent response from agent\n",
    "    current_information: str = \"\"  # All collected information, minimized for entry into the next agent\n",
    "    all_information: str = \"\"  # All collected information\n",
    "    all_information_index: None  # Index for the faiss index\n",
    "    all_information_database: None  # Database for the faiss index\n",
    "    current_report: str = \"\"  # Current report\n",
    "    final_summary: str = \"\"  # Final summary\n",
    "    prompts: List[str] = []\n",
    "    responses: List[str] = []\n",
    "    answering_agents: List[str] = []\n",
    "    variables: Dict[str, Any] = {}  # Clinician specific variables\n",
    "\n",
    "class TrialistState(MessagesState):\n",
    "    # Haoyang, add variables as you need, right now it's only global variables\n",
    "    next: Literal[\"Trialist\", \"__end__\"]\n",
    "    initial_question: str = \"\"  # Initial user question\n",
    "    predefined_next_agent: str = \"\"  # Predefined next agent\n",
    "    predefined_next_question: str = \"\"  # Predefined next question\n",
    "    sending_agent: str = \"\"  # Sending agent, agent who sent the most recent response\n",
    "    ongoing_question: str = \"\"  # Ongoing question\n",
    "    recent_response: str = \"\"  # Most recent response from agent\n",
    "    current_information: str = \"\"  # All collected information, minimized for entry into the next agent\n",
    "    all_information: str = \"\"  # All collected information\n",
    "    all_information_index: None  # Index for the faiss index\n",
    "    all_information_database: None  # Database for the faiss index\n",
    "    current_report: str = \"\"  # Current report\n",
    "    final_summary: str = \"\"  # Final summary\n",
    "    prompts: List[str] = []\n",
    "    responses: List[str] = []\n",
    "    answering_agents: List[str] = []\n",
    "    # Trialist specific variables\n",
    "    variables: Dict[str, Any] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MainSupervisor(state: TopAgentState):\n",
    "    \"\"\"\n",
    "    Main supervisor determines which top-level agent to delegate the task to.\n",
    "    Currently contains temporary code to delegate tasks dynamically.\n",
    "    \"\"\"\n",
    "\n",
    "    # Add ongoing question for context of most recent response, if question exists\n",
    "    if state['ongoing_question'] != \"\":  # fix bug\n",
    "        state['current_information'] = state['current_information'] + state['ongoing_question']\n",
    "\n",
    "    if state[\"recent_response\"] != \"\":\n",
    "        state[\"current_information\"] = state[\"current_information\"] + state[\"recent_response\"]\n",
    "    \n",
    "    # Summarize current_information if it is larger than 100000 tokens\n",
    "    if len(state[\"current_information\"].split()) > 100000:\n",
    "        state['all_information'] = state['all_information'] + state['current_information']\n",
    "        # Use an LLM to summarize the current_information into something smaller, keeping the most relevant details for the initial_question\n",
    "        prompt = f\"\"\"\n",
    "        You are an expert in summarizing information. Summarize the CURRENT INFORMATION into a smaller summary.\n",
    "        Keep the most relevant details for the INITIAL QUESTION.\n",
    "        Provide ONLY your answer and NOTHING else.\n",
    "        CURRENT INFORMATION: {state[\"current_information\"]}\n",
    "        INITIAL QUESTION: {state[\"initial_question\"]}\n",
    "        ANSWER:\n",
    "        \"\"\"\n",
    "        state['answering_agents'].append(\"Supervisor\")\n",
    "        state['prompts'].append(prompt)\n",
    "        response = llm.invoke(prompt, use_cache=False)\n",
    "        state['responses'].append(response)\n",
    "\n",
    "        print(state['answering_agents'][-1])\n",
    "        print(state['responses'][-1])\n",
    "        print(\"------------------------------------------\")\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "        state[\"current_information\"] = response.split(\"<|eot_id|>\")[0].strip()\n",
    "\n",
    "        RAW_KNOWLEDGE_BASE_ALL_INFO = [\n",
    "            Document(page_content=state['all_information'])\n",
    "        ]\n",
    "\n",
    "        # Splitting documents into chunks\n",
    "        docs_processed_all_info = utils.split_documents(\n",
    "            256,  # We choose a chunk size adapted to our model\n",
    "            RAW_KNOWLEDGE_BASE_ALL_INFO,\n",
    "            tokenizer_name=EMBEDDING_MODEL_NAME,\n",
    "        )\n",
    "\n",
    "        docs_content_all_info = []\n",
    "        for doc_processed in docs_processed_all_info:\n",
    "            docs_content_all_info.append(doc_processed.page_content)\n",
    "\n",
    "        # Encoding the documents\n",
    "        vectors_all_info = encoder.encode(docs_content_all_info)\n",
    "        df_database_all_info = pd.DataFrame(data=docs_content_all_info, columns=['text'])\n",
    "        vector_dimension = vectors_all_info.shape[1]\n",
    "        index_all_info = faiss.IndexFlatL2(vector_dimension)\n",
    "        faiss.normalize_L2(vectors_all_info)\n",
    "        index_all_info.add(vectors_all_info)\n",
    "        state['all_information_index'] = index_all_info\n",
    "        state['all_information_database'] = df_database_all_info\n",
    "    \n",
    "    if state[\"all_information\"] != \"\":\n",
    "        query = state[\"recent_response\"]\n",
    "        search_vector = encoder.encode(query)\n",
    "        _vector = np.array([search_vector])\n",
    "        faiss.normalize_L2(_vector)\n",
    "        k = index_all_info.ntotal\n",
    "        distances, ann = index_all_info.search(_vector, k=k)\n",
    "        results = pd.DataFrame({'distances': distances[0], 'ann': ann[0]})\n",
    "        merge = pd.merge(results, df_database_all_info, left_on='ann', right_index=True)\n",
    "        relevant_all_info_text = (\"\\n\\n\").join(list(merge['text'].values[0:3]))\n",
    "    else:\n",
    "        relevant_all_info_text = \"\"\n",
    "\n",
    "\n",
    "    # Check if state['Variables']['trialid'] exists\n",
    "    if 'trialid' not in state['variables']:\n",
    "        closest_trial_ids, closest_trial_id = get_most_relevant_clinical_trials(state[\"initial_question\"])\n",
    "        state['variables']['trialid'] = closest_trial_id\n",
    "        state['variables']['trialid_list'] = closest_trial_ids\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are the main supervisor. Based on the INPUT, RECENT RESPONSE, CURRENT INFORMATION, ADDITIONAL INFOMRATION, determine the next step in the workflow. \n",
    "\n",
    "    \n",
    "    INPUT: {state[\"initial_question\"]}\n",
    "\n",
    "    RECENT RESPONSE: {state[\"recent_response\"]}\n",
    "\n",
    "    SENDING AGENT: {state[\"sending_agent\"]}\n",
    "\n",
    "    CURRENT INFORMATION: {state[\"current_information\"]}\n",
    "\n",
    "    ADDITIONAL INFORMATION: {relevant_all_info_text}\n",
    "\n",
    "    PREDEFINED NEXT AGENT: {state[\"predefined_next_agent\"]}\n",
    "    \n",
    "\n",
    "    The possible next agents/steps are:\n",
    "    - Trialist: Create or edit the target trial protocol when and if needed.\n",
    "    - Informatician: Prepare and clean the data based on the target trial protocol and clinical literature. \\\n",
    "    The informatician needs a target trial and elibility criteria from the Trialist.\n",
    "    - Clinician: Provide research or medical advice to answer the question from a medical professional. \\\n",
    "    The clinician can also provide advice of if trial emulation design or trial emulation results are reasonable and if more exploration is needed.\n",
    "    - Statistician: Perform statistical analysis, machine learning, and or emulation to answer the question from computational experts. \\\n",
    "    This agent will perform processing, propensity score matching, estimating averate treatment effect, and Cox Proportional Hazards model to get results. \\\n",
    "    The statistician needs a cleaned dataset provided by the Informatician.\n",
    "\n",
    "    As mentioned, based on the INPUT, RECENT RESPONSE, CURRENT INFORMATION, ADDITIONAL INFOMRATION, determine the next step in the workflow. \n",
    "    If PREDEFINED NEXT AGENT is provided, delegate the task to that agent but come up with a request based on the CURRENT INFORMATION and RECENT RESPONSE.\n",
    "    If there is a previous response from the Clinican that includes results from trial emulation and a set of recommendations, \\\n",
    "    but the Informatician states that it is not possible to create a dataset that follows those recommendations, \\\n",
    "    the next step should \"End\".\n",
    "\n",
    "    Only provide the NAME of the next agent to execute. For example, if the next agent is Clinician, ouptut \"Clinician\".\n",
    "    In addition, provide a REQUEST for the next agent to execute. Output your answer as follows: \"NEXT: NextAgent, REQUEST: Your request here\". DO NOT PROVIDE ANY OTHER INFORMATION.\n",
    "    The text of the request should be entirely self-contained and not require the reference of CURRENT INFORMATION or any other context variables.\n",
    "\n",
    "    DO NOT PROVIDE ANY FORMATTING IN THE ANSWER.\n",
    "    \n",
    "    ANSWER:\n",
    "    \"\"\"\n",
    "\n",
    "    if state['final_summary'] != \"\":\n",
    "        if state['variables']['ec_optimization_results'] != \"\":\n",
    "            state[\"final_summary\"] = state[\"final_summary\"] + \"\\n Eligibility Criteria Optimization Results:\\n\" + state['variables']['ec_optimization_results']\n",
    "            state[\"next\"] = \"__end__\"\n",
    "            return state\n",
    "        elif state['variables']['ec_patients_powerset_dataframe'] is None:\n",
    "            state[\"next\"] = \"InformaticianAgentInvoker\"\n",
    "            state[\"predefined_next_agent\"] = \"Informatician\"\n",
    "            state[\"predefined_next_question\"] = \"Generate the required data for eligibility criteria optimization.\"\n",
    "            state[\"ongoing_question\"] = state[\"predefined_next_question\"]\n",
    "            state['answering_agents'].append(\"Supervisor\")\n",
    "            state['prompts'].append(prompt)\n",
    "            state['responses'].append(state[\"ongoing_question\"])\n",
    "\n",
    "            print(state['answering_agents'][-1])\n",
    "            print(state['responses'][-1])\n",
    "            print(\"------------------------------------------\")\n",
    "            print(\"\\n\\n\")\n",
    "\n",
    "            # Clearing predefined_next_agent\n",
    "            state[\"predefined_next_agent\"] = \"\"\n",
    "            state[\"predefined_next_question\"] = \"\"\n",
    "            # Clearing the recent response\n",
    "            state[\"recent_response\"] = \"\"\n",
    "            return state\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    if state[\"predefined_next_agent\"] != \"\" and state['predefined_next_question'] != \"\":\n",
    "        if state[\"predefined_next_agent\"] == \"Trialist\":\n",
    "            state[\"next\"] = \"TrialistInvoker\"\n",
    "        elif state[\"predefined_next_agent\"] == \"Informatician\":\n",
    "            state[\"next\"] = \"InformaticianAgentInvoker\"\n",
    "        elif state[\"predefined_next_agent\"] == \"Clinician\":\n",
    "            state[\"next\"] = \"ClinicianAgentInvoker\"\n",
    "        elif state[\"predefined_next_agent\"] == \"Statistician\":\n",
    "            state[\"next\"] = \"StatisticianAgentInvoker\"\n",
    "        state[\"ongoing_question\"] = state[\"predefined_next_question\"]\n",
    "        state['answering_agents'].append(\"Supervisor\")\n",
    "        state['prompts'].append(prompt)\n",
    "        state['responses'].append(state[\"ongoing_question\"])\n",
    "        \n",
    "        print(state['answering_agents'][-1])\n",
    "        print(state['responses'][-1])\n",
    "        print(\"------------------------------------------\")\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "        # Clearing predefined_next_agent\n",
    "        state[\"predefined_next_agent\"] = \"\"\n",
    "        state[\"predefined_next_question\"] = \"\"\n",
    "        # Clearing the recent response\n",
    "        state[\"recent_response\"] = \"\"\n",
    "        return state\n",
    "\n",
    "    elif state[\"predefined_next_agent\"] == \"Reviewer\":\n",
    "        state[\"next\"] = \"ClinicianAgentInvoker\"\n",
    "        state[\"ongoing_question\"] = \"Review the report and provide recommendations.\"\n",
    "        state['answering_agents'].append(\"Supervisor\")\n",
    "        state['prompts'].append(\"Given the report, what is the next step?\")\n",
    "        state['responses'].append(state[\"ongoing_question\"])\n",
    "        \n",
    "        print(state['answering_agents'][-1])\n",
    "        print(state['responses'][-1])\n",
    "        print(\"------------------------------------------\")\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "        # Clearing predefined_next_agent\n",
    "        state[\"predefined_next_agent\"] = \"\"\n",
    "        state[\"predefined_next_question\"] = \"\"\n",
    "        # Clearing the recent response\n",
    "        state[\"recent_response\"] = \"\"\n",
    "        return state\n",
    "\n",
    "    else:\n",
    "        state['answering_agents'].append(\"Supervisor\")\n",
    "        state['prompts'].append(prompt)\n",
    "        response = llm.invoke(prompt, use_cache=False)\n",
    "        state['responses'].append(response)\n",
    "        \n",
    "        print(state['answering_agents'][-1])\n",
    "        print(state['responses'][-1])\n",
    "        print(\"------------------------------------------\")\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "    try:\n",
    "        next_agent_response = response.split(\"NEXT:\")[1].split(\"REQUEST:\")[0].strip()\n",
    "        request = response.split(\"REQUEST:\")[1].strip()\n",
    "    except:\n",
    "        next_agent_response = \"End\"\n",
    "        request = \"\"\n",
    "    \n",
    "    # Setting question for next agent, this ongoing_question is only set by the MainSupervisor\n",
    "    # Preserve the clinician response\n",
    "    if state['sending_agent'] == 'Clinician':\n",
    "        state[\"ongoing_question\"] = \"Clinician Answer: \" + state[\"recent_response\"] + \" Request: \" + request\n",
    "    else:\n",
    "        state[\"ongoing_question\"] = request\n",
    "    \n",
    "    # Clearing predefined_next_agent\n",
    "    state[\"predefined_next_agent\"] = \"\"\n",
    "    state[\"predefined_next_question\"] = \"\"\n",
    "\n",
    "    # Clearing the recent response\n",
    "    state[\"recent_response\"] = \"\"\n",
    "    \n",
    "    if \"Clinician\" in next_agent_response:\n",
    "        state[\"next\"] = \"ClinicianAgentInvoker\"\n",
    "    elif \"Statistician\" in next_agent_response:\n",
    "        state[\"next\"] = \"StatisticianAgentInvoker\"\n",
    "    elif \"Trialist\" in next_agent_response:\n",
    "        state[\"next\"] = \"TrialistInvoker\"\n",
    "    elif \"Informatician\" in next_agent_response:\n",
    "        state[\"next\"] = \"InformaticianAgentInvoker\"\n",
    "    elif \"End\" in next_agent_response:\n",
    "        if state[\"final_summary\"] == \"\":\n",
    "            if state[\"current_report\"] != \"\":\n",
    "                state[\"final_summary\"] = state[\"current_report\"]\n",
    "        state[\"next\"] = \"__end__\"\n",
    "    else:\n",
    "        state[\"next\"] = \"__end__\"\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "def Trialist(state: TrialistState):\n",
    "    \n",
    "    state['sending_agent'] = \"Trialist\"\n",
    "    \n",
    "    if 'trialid' not in state['variables']:\n",
    "        state['recent_response'] = f\"Trial ID is not specified.\"\n",
    "        state['sending_agent'] = \"Trialist\"\n",
    "        state['next'] = \"__end__\"\n",
    "        return state \n",
    "\n",
    "    trial_pkl_path = osp.join('trials', f\"{state['variables']['trialid']}\", f\"trial_info_{state['variables']['trialid']}.pkl\")\n",
    "    if not osp.exists(trial_pkl_path):\n",
    "        parse_clinical_trial(state['variables']['trialid'])\n",
    "    \n",
    "    # load trial info\n",
    "    with open(trial_pkl_path, \"rb\") as fin:\n",
    "        trial_info = pickle.load(fin)\n",
    "    \n",
    "    state['variables']['inclusion_criteria'] = [item.text for item in trial_info.inclusion_criteria]\n",
    "    state['variables']['exclusion_criteria'] = [item.text for item in trial_info.exclusion_criteria]\n",
    "    if len(trial_info.treatments) == 1:\n",
    "        state['variables']['treatment_definition'] = [item.text for item in trial_info.treatments]\n",
    "    elif len(trial_info.treatments) == 2:\n",
    "        state['variables']['treatment_definition'] = [f\"Treated: {trial_info.treatments[0].text}; Control: {trial_info.treatments[1].text}\"]\n",
    "    else:\n",
    "        assert False, \"Trialist should only have two treatments, one for treated and one for control.\"\n",
    "    state['variables']['outcome_definition'] = [item.text for item in trial_info.outcomes]\n",
    "\n",
    "    state[\"recent_response\"] = f\"\"\"\n",
    "        Parsed trial information is as follows and no need to go back to Trialist again even Clinician recommended.\n",
    "        Inclusion Criteria: {state['variables']['inclusion_criteria']};\n",
    "        Exclusion Criteria: {state['variables']['exclusion_criteria']};\n",
    "        Treatment Definition: {state['variables']['treatment_definition']};\n",
    "        Outcome Definition: {state['variables']['outcome_definition']};\n",
    "    \"\"\"\n",
    "    state['prompts'].append(\"Come up with the information about the trial to emulate.\")\n",
    "    state['responses'].append(state[\"recent_response\"])\n",
    "    state['answering_agents'].append(\"Trialist\")\n",
    "    \n",
    "    print(state['answering_agents'][-1])\n",
    "    print(state['responses'][-1])\n",
    "    print(\"------------------------------------------\")\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    state[\"next\"] = \"__end__\"\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "def Clinician(state: ClinicianState):\n",
    "    \"\"\"Medical professional agent.\"\"\"\n",
    "    \"\"\"Can look at pubmed abstracts and own expertise to answer questions\"\"\"\n",
    "\n",
    "    # Getting relevant text from pdf literature\n",
    "    if 'compiled_relevant_texts' not in state['variables']:\n",
    "        state['variables']['compiled_relevant_texts'] = pd.DataFrame(columns=['text'])\n",
    "    combined_texts, state = get_relevant_info_from_lit_and_pubmed(state)\n",
    "\n",
    "    if 'number_time_provided_recs' not in state['variables']:\n",
    "        state['variables']['number_time_provided_recs'] = 0\n",
    "\n",
    "    if state['ongoing_question'] == \"Review the report and provide recommendations.\":\n",
    "        if state['variables']['number_time_provided_recs'] >= 2:\n",
    "            response = \"Clinician has already provided recommendations two times. No further recommendations will be provided. CREATE FINAL REPORT\"\n",
    "        else:\n",
    "            review_prompt = f\"\"\"\n",
    "            You are a medical professional with expertise in a wide variety of medical fields. You are tasked with analyzing the given reports and providing recommendations \\\n",
    "            Based on the REPORT and your own expertise, you need to DECIDE whether to provide recommendations or a final report. \\\n",
    "\n",
    "            REPORT: {state['current_report']}\n",
    "\n",
    "            ---------------------\n",
    "\n",
    "            Follow the tips:\n",
    "            - If you believe the results are satisfactory, and the discrepancies are minor or cannot be solved, opt to NOT give recommendations.\n",
    "            - Your theoretical recommendations should be able to be carried out within the MIMIC-IV database and no other external data. If they cannot, then do NOT choose to give recommendations.\n",
    "            - If the theoretical recommendations are too broad or create too much change, just opt to include them as Limitations in the final report. \\\n",
    "            - Prefer creating a final report if there are no urgent changes needed. \\\n",
    "\n",
    "            The Expected Structure Output Should Be Either:\n",
    "            \"CREATE FINAL REPORT\"\n",
    "            or\n",
    "            \"PROVIDE RECOMMENDATIONS\"\n",
    "\n",
    "            DO NOT PROVIDE ANY FORMATTING IN THE ANSWER.\n",
    "            \"\"\"\n",
    "\n",
    "            state['answering_agents'].append(\"Clinician\")\n",
    "            state['prompts'].append(review_prompt)\n",
    "            response = llm.invoke(review_prompt, use_cache=False)\n",
    "            state['responses'].append(response)\n",
    "\n",
    "            print(state['answering_agents'][-1])\n",
    "            print(state['responses'][-1])\n",
    "            print(\"------------------------------------------\")\n",
    "            print(\"\\n\\n\")\n",
    "\n",
    "        if \"CREATE FINAL REPORT\" in response:\n",
    "            final_review_prompt = f\"\"\"\n",
    "                You are a clinician with medical expertise. Create a final report based on the CURRENT REPORT. \\\n",
    "                Provide a comprehensive summary of the trial emulation process, results, and recommendations for future research. \\\n",
    "                \n",
    "                You are given the following: \n",
    "\n",
    "                CURRENT REPORT: {state['current_report']}\n",
    "                \n",
    "                You need output a complete report. It should include a clear and informative title page that outlines the studys title, \\\n",
    "                followed by an abstract summarizing the objective, methods, key results, and conclusions. The introduction should provide background \\\n",
    "                on the original trial, the rationale for emulation, the study objectives, and key challenges. A detailed description of the data sources is essential, \\\n",
    "                including the dataset characteristics, inclusion/exclusion criteria, and data quality assessments. The study design should outline how the emulation \\\n",
    "                mirrors the original trial, specifying eligibility criteria, treatment strategies, outcomes, and follow-up \\\n",
    "                periods. The statistical methods section should describe the causal inference approaches, confounder adjustment techniques, \\\n",
    "                outcome modeling, and sensitivity analyses. The results section should present baseline characteristics, treatment effect estimates, \\\n",
    "                comparisons with the original trial, and findings from sensitivity analyses, supported by tables and visualizations. In the discussion, \\\n",
    "                key findings should be interpreted in the context of consistency with the original trial, alongside the studys strengths, limitations, \\\n",
    "                and implications for future research. The conclusion should summarize the studys impact and contributions.\n",
    "\n",
    "                Expected Structure Output:\n",
    "                \"FINAL REPORT: <Your final report here>\"\n",
    "\n",
    "                Provide ONLY your answer and NOTHING else.\n",
    "                \"\"\"\n",
    "        \n",
    "            state['answering_agents'].append(\"Clinician\")\n",
    "            state['prompts'].append(final_review_prompt)\n",
    "            response = llm.invoke(final_review_prompt, use_cache=False)\n",
    "            state['responses'].append(response)\n",
    "\n",
    "            print(state['answering_agents'][-1])\n",
    "            print(state['responses'][-1])\n",
    "            print(\"------------------------------------------\")\n",
    "            print(\"\\n\\n\")\n",
    "\n",
    "            if \"FINAL REPORT\" in response:\n",
    "                response = response.split(\"FINAL REPORT\")[1].strip()\n",
    "                state['final_summary'] = response\n",
    "                state[\"recent_response\"] = response.strip()\n",
    "                state['sending_agent'] = \"Clinician\"\n",
    "                state[\"next\"] = \"__end__\"\n",
    "                return state\n",
    "\n",
    "\n",
    "        if \"PROVIDE RECOMMENDATIONS\" in response:\n",
    "            state['variables']['number_time_provided_recs'] += 1\n",
    "            recommendations_prompt = f\"\"\"\n",
    "            You are a clinician with medical expertise. Based on the CURRENT REPORT and CURRENT INFORMATION, provide recommendations for further analysis or changes to the trial emulation process. \\\n",
    "            Your recommendations should be specific, actionable, and relevant to the trial emulation process. \\\n",
    "\n",
    "            CURRENT REPORT: {state['current_report']}\n",
    "\n",
    "\n",
    "            CURRENT INFORMATION: {state['current_information']}\n",
    "\n",
    "\n",
    "            Here is an expected **Example Output**:\n",
    "            '''\n",
    "            RECOMMENDATIONS:\n",
    "            <Treatment_Effect>Original Hazard Ratio: 0.82 (95% CI: 0.70 - 0.95)</Treatment_Effect>   \n",
    "            <Comparison>Results are consistent within the confidence intervals, suggesting alignment.</Comparison>   \n",
    "            <Potential_Discrepancy>Minor difference in point estimates</Potential_Discrepancy>   \n",
    "            <Reason>Differences may stem from variations in patient populations or eligibility criteria in our trial emulation.</Reason>   \n",
    "            <Recommendation>No major concerns. Proceed with the current analysis, but include a note discussing potential sources of discrepancy.</Recommendation> \n",
    "            Reasons of discrepancies: \n",
    "            1. Eligibility Criteria Mismatch: \n",
    "            Our trial emulation's EC is less restrictive compared to the Original Trial. \n",
    "            <Solution>Add additional eligibility criteria to better align with the Original Trial, such as stricter temporal conditions or exclusion criteria.</Solution> \n",
    "            2. Causal Estimation Method Inaccuracy: \n",
    "            The current causal inference method might not fully account for confounding.   \n",
    "            <Solution>Consider switching causal estimation methods, such as propensity score matching and G-computation.</Solution> \n",
    "            3. The smaller sample size in the emulated trial might lead to unstable estimates.   \n",
    "            <Solution>Relax certain eligibility criteria (e.g., extend temporal windows) to increase sample size while monitoring covariate balance.</Solution> \n",
    "            PREDEFINED AGENT: Informatician\n",
    "            '''\n",
    "\n",
    "            If there are recommendations, you may want to send the recommendations to **ONE** specific agent. \\\n",
    "            If so, provide the agent's name at the end of the output (in addition to the answer) as PREDEFINED AGENT: AgentName \\\n",
    "            Here are the possible agents: \\\n",
    "            - Trialist: Create or edit the target trial protocol when and if needed. \\\n",
    "            - Informatician: Prepare and clean the data based on the target trial protocol, clinical literature. The clinician deals with curating a new dataset based on recommendations. \\\n",
    "            - Statistician: Perform statistical analysis, machine learning, and or emulation to answer the question from computational experts. \\\n",
    "\n",
    "            Follow the tips:\n",
    "            - If your recommendations require the creation of a new dataset, the PREDEFINED AGENT should be the Informatician.\n",
    "            -- Examples of creating a new dataset:\n",
    "                --- Incorporate subgroup analyses focusing on patient stratifications such as illness severity \n",
    "                --- Relax Temporal Eligibility Criteria for Trial Emulation\n",
    "            - If you believe the results are satisfactory, and the discrepancies are minor or cannot be solved, make a complete report including the motivation, background, results, and discussion of discrepancies.\n",
    "            - Your recommendations should be able to be carried out within the MIMIC-IV database and no other external data. \\\n",
    "            - If the recommendations are too broad or create too much change, just include them as Limitations in the final report. \\\n",
    "            - Prefer putting reccomendations in the Limitations. \\\n",
    "\n",
    "            Provide ONLY your answer and NOTHING else.\n",
    "            \"\"\"\n",
    "\n",
    "            state['answering_agents'].append(\"Clinician\")\n",
    "            state['prompts'].append(recommendations_prompt)\n",
    "            response = llm.invoke(recommendations_prompt, use_cache=False)\n",
    "            state['responses'].append(response)\n",
    "\n",
    "            print(state['answering_agents'][-1])\n",
    "            print(state['responses'][-1])\n",
    "            print(\"------------------------------------------\")\n",
    "            print(\"\\n\\n\")\n",
    "\n",
    "            if \"PREDEFINED AGENT\" in response:\n",
    "                predefined_next_agent = response.split(\"PREDEFINED AGENT: \")[1].split(\"\\n\")[0].strip()\n",
    "                response = response.split(\"PREDEFINED AGENT: \")[0]\n",
    "\n",
    "                # Ask llm to provide a structured list of steps for the predefined agent to take\n",
    "                prompt = f\"\"\"\n",
    "                Provide a structured list of steps for the {predefined_next_agent} to take based on the given response.\n",
    "\n",
    "                The response is as follows:\n",
    "                {response}\n",
    "\n",
    "                Please provide a clear and concise list of steps for the {predefined_next_agent} to follow in response to the given information.\n",
    "\n",
    "                Structure your response as follows:\n",
    "                1. Step 1\n",
    "                2. Step 2\n",
    "                3. Step 3\n",
    "                ...\n",
    "\n",
    "                Provide ONLY your answer and NOTHING else.\n",
    "                \"\"\"\n",
    "\n",
    "                state['answering_agents'].append(\"Clinician\")\n",
    "                state['prompts'].append(prompt)\n",
    "                steps_response = llm.invoke(prompt, use_cache=False)\n",
    "                state['responses'].append(response)\n",
    "\n",
    "                print(state['answering_agents'][-1])\n",
    "                print(state['responses'][-1])\n",
    "                print(\"------------------------------------------\")\n",
    "                print(\"\\n\\n\")\n",
    "\n",
    "                state['predefined_next_agent'] = predefined_next_agent\n",
    "                state['predefined_next_question'] = steps_response\n",
    "                \n",
    "                state[\"recent_response\"] = steps_response.strip()\n",
    "                state['sending_agent'] = \"Clinician\"\n",
    "                state[\"next\"] = \"__end__\"\n",
    "\n",
    "        \n",
    "        state[\"recent_response\"] = response.strip()\n",
    "        state['sending_agent'] = \"Clinician\"\n",
    "        state[\"next\"] = \"__end__\"\n",
    "\n",
    "        return state\n",
    "\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a medical professional with expertise in a wide variety of medical fields. \\\n",
    "    Based on the QUESTION and MEDICAL LITERATURE, and your own expertise provide an answer to the question. \\\n",
    "    You might get requests (INPUT) about doing research, providing guidance of how to change trial design, or providing medical advice.\n",
    "    You may want to send the answer to a specific agent. It will likely be the SENDING AGENT. \\\n",
    "    If so, provide the agent's name at the end of the output (in addition to the answer) as PREDEFINED AGENT: AgentName \\\n",
    "    Here are the possible agents: \\\n",
    "    - Statistician: Perform statistical analysis, machine learning, and or emulation to answer the question from computational experts. \\\n",
    "    - Trialist: Create or edit the target trial protocol when and if needed. \\\n",
    "    - Informatician: Prepare and clean the data based on the target trial protocol and clinical literature. \\\n",
    "   \n",
    "    \n",
    "    You might get other questions as well that are not covered in the examples above. Provide the answers in a structured format. \\\n",
    "    The output will need to be parsed by the next agent to extract the answer. \\\n",
    "    The output for the provided examples should be structured as shown in the above examples.\n",
    "    ONLY provide the answer to the question. DO NOT provide any other information and be concise.\n",
    "\n",
    "    DO NOT PROVIDE ANY ADDITIONAL FORMATTING IN THE ANSWER.\n",
    "\n",
    "    SENDING AGENT: {state[\"sending_agent\"]}\n",
    "\n",
    "    MEDICAL LITERATURE: {combined_texts}\n",
    "\n",
    "    QUESTION: {state[\"ongoing_question\"]}\n",
    "\n",
    "\n",
    "    ANSWER:\n",
    "    \"\"\"\n",
    "\n",
    "    state['answering_agents'].append(\"Clinician\")\n",
    "    state['prompts'].append(prompt)\n",
    "    response = llm.invoke(prompt, use_cache=False)\n",
    "    state['responses'].append(response)\n",
    "\n",
    "    print(state['answering_agents'][-1])\n",
    "    print(state['responses'][-1])\n",
    "    print(\"------------------------------------------\")\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    if \"PREDEFINED AGENT: \" in response:\n",
    "        state[\"predefined_next_agent\"] = response.split(\"PREDEFINED AGENT: \")[1].split(\"\\n\")[0].strip()\n",
    "        if \"ANSWER:\" in response:\n",
    "            response = response.split(\"ANSWER: \")[1].split(\"PREDEFINED AGENT: \")[0]\n",
    "        response = response.split(\"PREDEFINED AGENT: \")[0]\n",
    "        state[\"predefined_next_question\"] = response\n",
    "\n",
    "    if \"INCLUDED COVARIATES\" in response:  # jump back to Informatician ### QUESTION\n",
    "        state['predefined_next_question'] = response\n",
    "        state['predefined_next_agent'] == \"Informatician\"\n",
    "\n",
    "    if (\"STRATIFY ON\" in response) and (\"LEVEL\" in response):\n",
    "        # Replace \"**STRATIFY ON**\" with \"STRATIFY ON\"\n",
    "        response = response.replace(\"**STRATIFY ON**\", \"STRATIFY ON\")\n",
    "        response = response.replace(\"**LEVEL**\", \"LEVEL\")\n",
    "        response = \"SUBGROUP ANALYSIS\\n\\n\" + response\n",
    "        state['predefined_next_question'] = response\n",
    "        state['predefined_next_agent'] = \"Statistician\"\n",
    "\n",
    "    if \"UNBALANCED COVARIATES\" in response:\n",
    "        response = response.replace(\"**UNBALANCED COVARIATES**\", \"UNBALANCED COVARIATES\")\n",
    "        state['predefined_next_question'] = response\n",
    "        state['predefined_next_agent'] = \"Statistician\"\n",
    "    \n",
    "    if \"ANSWER:\" in response:\n",
    "        response = response.split(\"ANSWER: \")[1].strip()\n",
    "\n",
    "    state[\"recent_response\"] = response.strip()\n",
    "\n",
    "    if \"FINAL SUMMARY\" in state[\"recent_response\"]:\n",
    "        state[\"final_summary\"] = state[\"recent_response\"].split(\"FINAL SUMMARY:\")[1].strip()\n",
    "    \n",
    "    state['sending_agent'] = \"Clinician\"\n",
    "    state[\"next\"] = \"__end__\"\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "def Informatician(state: InformaticianState):\n",
    "    \"\"\"\n",
    "    Informatician Agent to process raw CSV data, compute required variables,\n",
    "    and prepare a cleaned DataFrame for trial emulation.\n",
    "    \"\"\"\n",
    "\n",
    "    state['sending_agent'] = \"Informatician\"\n",
    "\n",
    "    # 1. check the availability of required variables\n",
    "    if 'inclusion_criteria' not in state['variables'] or state['variables']['inclusion_criteria'] is None:\n",
    "        state['recent_response'] = \"inclusion_criteria is not available.\"\n",
    "        state['sending_agent'] = \"Informatician\"\n",
    "        state['next'] = \"__end__\"\n",
    "        return state\n",
    "    if 'exclusion_criteria' not in state['variables'] or state['variables']['exclusion_criteria'] is None:\n",
    "        state['recent_response'] = \"exclusion_criteria is not available.\"\n",
    "        state['sending_agent'] = \"Informatician\"\n",
    "        state['next'] = \"__end__\"\n",
    "        return state\n",
    "    if 'treatment_definition' not in state['variables'] or state['variables']['treatment_definition'] is None:\n",
    "        state['recent_response'] = \"treatment_definition is not available.\"\n",
    "        state['sending_agent'] = \"Informatician\"\n",
    "        state['next'] = \"__end__\"\n",
    "        return state\n",
    "    if 'outcome_definition' not in state['variables'] or state['variables']['outcome_definition'] is None:\n",
    "        state['recent_response'] = \"outcome_definition is not available.\"\n",
    "        state['sending_agent'] = \"Informatician\"\n",
    "        state['next'] = \"__end__\"\n",
    "        return state    \n",
    "    \n",
    "\n",
    "    # 2. load df and cols information \n",
    "    trial_info = {\n",
    "        'treatment_definition': [clean_ec(_) for _ in state['variables']['treatment_definition']],\n",
    "        'outcome_definition': [clean_ec(_) for _ in state['variables']['outcome_definition']],\n",
    "        'inclusion_criteria': [clean_ec(_) for _ in state['variables']['inclusion_criteria']],\n",
    "        'exclusion_criteria': [clean_ec(_) for _ in state['variables']['exclusion_criteria']]\n",
    "    }\n",
    "    informatician = InformaticianTool(trialID=state['variables']['trialid'], trial_info=trial_info, llm=llm, dataset=state['variables'].get('dataset', None), root_dir='./trials')\n",
    "    df, cols_dict = informatician.get_df_dict()\n",
    "    # df_notes = informatician.get_notes()\n",
    "    \n",
    "    state['variables']['cleaned_dataframe'] = df\n",
    "    state['variables']['cols_dict'] = cols_dict\n",
    "    # state['variables']['Covariates'] = cols_dict['covariate_cols']\n",
    "    state['variables']['Treatment'] = cols_dict['treatment_cols']\n",
    "    state['variables']['Outcome'] = cols_dict['outcome_cols']\n",
    "    state['variables']['Duration']  = cols_dict['duration_cols']\n",
    "    state['variables']['patient_id']  = cols_dict['patient_id']\n",
    "\n",
    "    # for handling immortal_time_bias  \n",
    "    state['variables']['treatment_time_cols']  = cols_dict['treatment_time_cols'] if 'treatment_time_cols' in cols_dict else None\n",
    "\n",
    "    # Defining state['variables']['ec_patients_powerset_dataframe'] for eligibility criteria optimization for the future\n",
    "    state['variables']['ec_patients_powerset_dataframe'] = None\n",
    "\n",
    "    # 3. prepare the eligibility criteria optimization\n",
    "    if 'ELIGIBILITY CRITERIA OPTIMIZATION' in state[\"ongoing_question\"].upper():\n",
    "\n",
    "        dict_ec = informatician.get_permuted_ec_for_optimize()\n",
    "        state['variables']['patient_id'] = cols_dict['patient_id']\n",
    "        state['variables']['big_dataframe'] = dict_ec['variables']['big_dataframe']\n",
    "        state['variables']['ec_patients_powerset_dataframe'] = dict_ec['variables']['ec_patients_powerset_dataframe']\n",
    "\n",
    "        state['sending_agent'] = \"Informatician\"\n",
    "        state[\"predefined_next_question\"] = \"Hi Statistician, you need to conduct eligibility criteria optimization.\"\n",
    "        state[\"predefined_next_agent\"] = \"Statistician\"\n",
    "        state[\"next\"] = \"__end__\"\n",
    "        return state\n",
    "\n",
    "    # 4. process included covariates\n",
    "    if 'included_covariates' not in state['variables'] or state['variables']['included_covariates'] is None: # ask Clinician to set covariates that will be built for dataframe\n",
    "        \n",
    "        if (\"flag_asked_clinician_include_covariates\" in state[\"variables\"] and state[\"variables\"][\"flag_asked_clinician_include_covariates\"]) or 'INCLUDED COVARIATES' in state['predefined_next_question'] and state['sending_agent'] == \"Clinician\": # process response from Clinician\n",
    "            covariates_to_include = []\n",
    "            try:\n",
    "                covariates_to_include = state['recent_response'].split(\"INCLUDED COVARIATES:\")[1].strip().split(\",\")\n",
    "            except:\n",
    "                pass\n",
    "            covariates_to_include = [_.strip() for _ in covariates_to_include]\n",
    "            state['variables']['included_covariates'] = [col for col in cols_dict['covariate_cols'] if col in covariates_to_include] # select the covariates that are in the list\n",
    "            state[\"predefined_next_question\"] = \"\"\n",
    "\n",
    "        else:  # ask Clinician\n",
    "            state[\"variables\"][\"flag_asked_clinician_include_covariates\"] = True\n",
    "            state['sending_agent'] = \"Informatician\"\n",
    "            state[\"predefined_next_question\"] = f\"\"\"\n",
    "                Hi Clinician, you need to answer which covariates should be used for build the dataframe for analysis. \n",
    "                \n",
    "                You need to select a subset or the entire set (as MORE as possible) from the available covariate list that can serve as the covariates for samples in the dataset used for conducting trial emulation. After you\n",
    "\n",
    "                The available covariates are: {cols_dict['covariate_cols']}\n",
    "\n",
    "                In your answer, you need to include two lines. In the first line, you need to provide a list of the covariates to be selected in the following format:\n",
    "                INCLUDED COVARIATES: <Covariate1>,<Covariate2>,<Covariate3> ... where <Covariate1> is the covariate name. And then in the second line, uou also need to specify the PREDEFINED AGENT to be Informatician, namely output \"PREDEFINED AGENT: Informatician\"\n",
    "                Remember that you do not output any other context in the output except these two lines.\n",
    "\n",
    "                Do not change the covariate name in your answer and only use the covariate name that is shown in the available covariates that I give to you.\n",
    "                \n",
    "                Do **not** include additional information or context in the response. Do not provide an explanations or reasoning for the selection of covariates. ONLY provide the Output in Expected Structure Output.\n",
    "\n",
    "                Expected Structure Output:\n",
    "                INCLUDED COVARIATES: <Covariate1>|<Covariate2>|<Covariate3>|<Covariate4>|<Covariate5>\n",
    "                PREDEFINED AGENT: Informatician\n",
    "            \"\"\"\n",
    "            state['predefined_next_agent'] = \"Clinician\"\n",
    "            state[\"next\"] = \"__end__\"\n",
    "            return state\n",
    "\n",
    "    state['variables']['Covariates'] = state['variables']['included_covariates']\n",
    "\n",
    "    state[\"recent_response\"] = f\"\"\"\n",
    "        The built dataframe is ready for analyzing in Statistician. \n",
    "    \"\"\"\n",
    "    state['predefined_next_agent'] = \"Statistician\"\n",
    "    state['predefined_next_question'] = \"Conduct statistical analysis using the data provided.\"\n",
    "\n",
    "    # Quality check\n",
    "    if state['variables']['Covariates'] is None or len(state['variables']['Covariates']) <= 3:\n",
    "        state['variables']['Covariates'] = cols_dict['covariate_cols']\n",
    "        state[\"recent_response\"] += f\"\"\"The selection or adjustment on covariates failed or very few covariates are recommended, so Informatician includes all covariates, so that do not go back to Informatician again even being recommended by the others.\"\"\"\n",
    "\n",
    "    # loop check\n",
    "    if 'cnt_informatician_response' not in state['variables'] or state['variables']['cnt_informatician_response'] is None:\n",
    "        state['variables']['cnt_informatician_response'] = 0\n",
    "    else:\n",
    "        state['variables']['cnt_informatician_response'] += 1\n",
    "    if state['variables']['cnt_informatician_response'] > 10:  # TODO: might change in the future\n",
    "        state[\"recent_response\"] += f\"\"\"The Informatician has reach the maximum number of response so that do not go back to Informatician again even being recommended by the others.\"\"\"\n",
    "\n",
    "    # save responses\n",
    "    state['prompts'].append(\"Create the dataframe for the trial emulation.\")\n",
    "    state['responses'].append(state[\"recent_response\"])\n",
    "    state['answering_agents'].append(\"Statistician\")\n",
    "\n",
    "    print(state['answering_agents'][-1])\n",
    "    print(state['responses'][-1])\n",
    "    print(\"------------------------------------------\")\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    state[\"next\"] = \"__end__\"\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "def Statistician(state: StatisticianState):\n",
    "    \"\"\"\n",
    "    Enhanced Statistician Agent to build trial emulation protocol, select the best covariate balancing method,\n",
    "    conduct survival analysis using various models, and generate relevant result plots.\n",
    "    \"\"\"\n",
    "\n",
    "    list_of_outcomes = state['variables']['Outcome']\n",
    "    list_of_durations = state['variables']['Duration']\n",
    "    list_of_outcomes = [outcome.lower() for outcome in list_of_outcomes]\n",
    "    list_of_durations = [duration.lower() for duration in list_of_durations]\n",
    "    \n",
    "    outcome_definitions = state['variables']['outcome_definition'] + [_.replace('adverseevent_', '') for _ in state['variables']['Outcome'] if 'adverseevent' in _]\n",
    "    state['variables']['outcome_definitions'] = outcome_definitions\n",
    "\n",
    "    if 'completed_outcomes' not in state['variables']:\n",
    "        state['variables']['completed_outcomes'] = []\n",
    "\n",
    "\n",
    "    # ELIGIBILITY CRITERIA OPTIMIZATION\n",
    "    if 'ec_optimization_results' not in state['variables']:\n",
    "        state['variables']['ec_optimization_results'] = \"\"\n",
    "\n",
    "    if \"ELIGIBILITY CRITERIA OPTIMIZATION\" in state['ongoing_question'].upper():\n",
    "        shapley_value, name_rules, hrs_for_all_combos = perform_ec_optmization(state)\n",
    "        ec_optimixation_str = \"\"\n",
    "        for i in range(len(name_rules)):\n",
    "            if shapley_value[i] > 0:\n",
    "                ec_optimixation_str += f\"{name_rules[i]}: is associated with an increase in HR, with a SHAP value of {shapley_value[i]}\\n\"\n",
    "            elif shapley_value[i] < 0:\n",
    "                ec_optimixation_str += f\"{name_rules[i]}: is associated with a decrease in HR, with a SHAP value of {shapley_value[i]}\\n\"\n",
    "            else:\n",
    "                ec_optimixation_str += f\"{name_rules[i]}: has no association with HR, with a SHAP value of {shapley_value[i]}\\n\"\n",
    "        state['variables']['ec_optimization_results'] = ec_optimixation_str\n",
    "        state['variables']['ec_shapley_value'] = shapley_value\n",
    "        state['variables']['ec_name_rules'] = name_rules\n",
    "        state['variables']['flag_ec_optimization_performed'] = True\n",
    "        state['next'] = \"__end__\"\n",
    "        return state\n",
    "\n",
    "    \n",
    "    # Initiating loop for initial analysis for each outcome\n",
    "    if 'current_outcome_index' not in state['variables']:\n",
    "        # If current_outcome_index is not in state, initialize it to 0\n",
    "        state['variables']['current_outcome_index'] = 0\n",
    "\n",
    "    while state['variables']['current_outcome_index'] < len(list_of_outcomes):\n",
    "\n",
    "        list_of_outcome = list_of_outcomes[state['variables']['current_outcome_index']]\n",
    "        list_of_duration = list_of_durations[state['variables']['current_outcome_index']]\n",
    "        outcome_definition = outcome_definitions[state['variables']['current_outcome_index']]\n",
    "\n",
    "        if isinstance(list_of_outcome, str):\n",
    "            list_of_outcome = [list_of_outcome]\n",
    "        if isinstance(list_of_duration, str):\n",
    "            list_of_duration = [list_of_duration]\n",
    "        if isinstance(outcome_definition, str):\n",
    "            outcome_definition = [outcome_definition]\n",
    "        \n",
    "        if list_of_outcome[0] not in state['variables']:\n",
    "            state['variables'][list_of_outcome[0]] = {}\n",
    "\n",
    "        \n",
    "        # SUBGROUP ANALYSIS\n",
    "        if \"SUBGROUP ANALYSIS\" in state['ongoing_question']:\n",
    "            state = _handle_subgroup_analysis(state, list_of_outcome, list_of_duration)\n",
    "            if state['next'] == \"__end__\":\n",
    "                return state\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        # NON SUBGROUP ANALYSIS\n",
    "        else:\n",
    "            # Load the cleaned DataFrame\n",
    "            if state['variables']['cleaned_dataframe'] is None:\n",
    "                state['recent_response'] = \"Cleaned DataFrame is not available.\"\n",
    "                state['sending_agent'] = \"Statistician\"\n",
    "                state['next'] = \"__end__\"\n",
    "                return state\n",
    "\n",
    "            dataframe = state['variables']['cleaned_dataframe']\n",
    "\n",
    "            # make all columns lower case\n",
    "            dataframe.columns = [col.lower() for col in dataframe.columns]\n",
    "\n",
    "            if 'ignore_covariates' not in state['variables'][list_of_outcome[0]]:\n",
    "                state['variables'][list_of_outcome[0]]['ignore_covariates'] = []\n",
    "            if 'add_covariates' not in state['variables'][list_of_outcome[0]]:\n",
    "                state['variables'][list_of_outcome[0]]['add_covariates'] = []\n",
    "            if 'drop_covariates' not in state['variables'][list_of_outcome[0]]:\n",
    "                state['variables'][list_of_outcome[0]]['drop_covariates'] = []\n",
    "\n",
    "            if 'number_times_request_drop_covariates' not in state['variables'][list_of_outcome[0]]:\n",
    "                state['variables'][list_of_outcome[0]]['number_times_request_drop_covariates'] = 0\n",
    "\n",
    "            # Based on the most recent response, determine if there are any covariates to drop\n",
    "            state, covariates_to_add, covariates_to_drop, covariates_to_ignore = _parse_covariate_modifications(state,\n",
    "                                                                                                                dataframe,\n",
    "                                                                                                                list_of_outcome,\n",
    "                                                                                                                llm)\n",
    "\n",
    "            ## Informatician needs to store the relevant column names as a list to state variables\n",
    "            list_of_covariates = state['variables']['Covariates']\n",
    "            list_of_treatment = state['variables']['Treatment']\n",
    "\n",
    "            # if list_of_treatment is a string, convert it to a list\n",
    "            if isinstance(list_of_treatment, str):\n",
    "                list_of_treatment = [list_of_treatment]\n",
    "\n",
    "            # make all elements in list_of_covariates lower case\n",
    "            list_of_covariates = [covariate.lower() for covariate in list_of_covariates]\n",
    "            # make all elements in list_of_treatment lower case\n",
    "            list_of_treatment = [treatment.lower() for treatment in list_of_treatment]\n",
    "            # make all elements in list_of_outcome lower case\n",
    "            list_of_outcome = [outcome.lower() for outcome in list_of_outcome]\n",
    "            # make all elements in list_of_duration lower case\n",
    "            list_of_duration = [duration.lower() for duration in list_of_duration]\n",
    "            \n",
    "            # Might need to change code in the future\n",
    "            # Drop the covariates if needed in list_of_covariates\n",
    "            new_list_of_covariates = []\n",
    "            for covariate in list_of_covariates:\n",
    "                if covariate.lower() not in covariates_to_drop:\n",
    "                    new_list_of_covariates.append(covariate)\n",
    "            list_of_covariates = new_list_of_covariates\n",
    "\n",
    "            # Add the covariates if needed in list_of_covariates\n",
    "            # Check if covariates_to_add is in the dataframe\n",
    "            final_covariates_to_add = []\n",
    "            for covariate in covariates_to_add:\n",
    "                if covariate in dataframe.columns:\n",
    "                    final_covariates_to_add.append(covariate)\n",
    "            list_of_covariates = list(set(list_of_covariates + final_covariates_to_add))\n",
    "\n",
    "            identifier_columns = ['subject_id', 'stay_id']\n",
    "\n",
    "            df, categorical_covariates, continuous_covariates, list_of_treatment, list_of_outcome, list_of_duration = clean_data(list_of_covariates,\n",
    "                                                                                                                                list_of_treatment,\n",
    "                                                                                                                                    list_of_outcome,\n",
    "                                                                                                                                    list_of_duration,\n",
    "                                                                                                                                    identifier_columns,\n",
    "                                                                                                                                        dataframe)\n",
    "            \n",
    "\n",
    "            # if there are any remaining columns with all NaN values, drop them\n",
    "            columns_with_nan = df.columns[df.isna().all()].tolist()\n",
    "            df.drop(columns=columns_with_nan, inplace=True)\n",
    "            # get overlap between columns_with_nan and list_of_covariates\n",
    "            columns_with_nan = [col for col in columns_with_nan if col in list_of_covariates]\n",
    "            state['variables'][list_of_outcome[0]]['drop_covariates'].extend(columns_with_nan)\n",
    "            state['variables'][list_of_outcome[0]]['drop_covariates'] = list(set(state['variables'][list_of_outcome[0]]['drop_covariates']))\n",
    "            covariates_to_drop = state['variables'][list_of_outcome[0]]['drop_covariates']\n",
    "            # Drop the covariates if needed in list_of_covariates\n",
    "            list_of_covariates = [covariate for covariate in list_of_covariates if covariate.lower() not in covariates_to_drop]\n",
    "            categorical_covariates = [covariate for covariate in categorical_covariates if covariate.lower() not in covariates_to_drop]\n",
    "            continuous_covariates = [covariate for covariate in continuous_covariates if covariate.lower() not in covariates_to_drop]\n",
    "\n",
    "\n",
    "            # Step 1: Build Trial Emulation Protocol\n",
    "            trial_protocol = build_trial_emulation_protocol(state)\n",
    "\n",
    "            break_flag = False\n",
    "            results = {}\n",
    "            plots = {}\n",
    "\n",
    "            # Step 2.5 Perform Cloning If Possible to Fix Immortal Time Bias\n",
    "            id_col = 'stay_id' # need to be changed in the future\n",
    "            df = adjust_immortal_time_bias(df, id_col, treat_time_col=list_of_treatment[0], outcome_col=list_of_outcome[0], followup_end_col=list_of_duration[0], covariates=categorical_covariates + continuous_covariates, stabilize=True)\n",
    "\n",
    "            # Step 2: Select Covariate Balancing Method\n",
    "            balancing_method, balancing_rationale, survival_methods, survival_rationale = select_methods(state, df, trial_protocol, list_of_treatment, list_of_outcome, list_of_duration)\n",
    "            state['variables'][list_of_outcome[0]]['balancing_method'] = balancing_method\n",
    "            state['variables'][list_of_outcome[0]]['balancing_rationale'] = balancing_rationale\n",
    "            state['variables'][list_of_outcome[0]]['survival_methods'] = survival_methods\n",
    "            state['variables'][list_of_outcome[0]]['survival_rationale'] = survival_rationale\n",
    "\n",
    "            # Step 3: Conduct Covariate Balancing\n",
    "            balanced_df, balancing_str_info, break_flag, results, scaler = apply_covariate_balancing(state, balancing_method, df, categorical_covariates, continuous_covariates, state['variables'][list_of_outcome[0]]['ignore_covariates'], list_of_treatment, list_of_outcome, outcome_definition, break_flag, results)\n",
    "            if scaler is not None:\n",
    "                # Inverse transform the balanced dataframe\n",
    "                balanced_df[continuous_covariates] = scaler.inverse_transform(balanced_df[continuous_covariates])\n",
    "\n",
    "            if break_flag and state['variables'][list_of_outcome[0]]['number_times_request_drop_covariates'] <= 5: # If the covariates are not balanced, stop the analysis\n",
    "                balances = results['Balances after performing balancing']\n",
    "                unbalanced_covariates = list(balances[balances > 0.1].index)\n",
    "                unbalanced_covariates_str = \"\"\n",
    "                for covariate in unbalanced_covariates:\n",
    "                    unbalanced_covariates_str += f\"<Unbalanced_Covariate>{covariate}</Unbalanced_Covariate>\\n\"\n",
    "                issue = f'''\n",
    "                Hi Clinician, I noticed the ratio of unbalanced covariates is quite high, which may impact the validity of our estimation results.  \n",
    "                Id like to ask for your input: Can we drop these unbalanced covariates if they are less clinically significant? \n",
    "                Alternatively, can you recommend substitute covariates that might capture similar clinical signals while improving balance?  \n",
    "                Here are the unbalanced covariates I identified: \n",
    "                {unbalanced_covariates_str}\n",
    "\n",
    "                Expected Structure Output:\n",
    "                UNBALANCED COVARIATES\n",
    "                <Unbalanced_Covariate>|<DROP or SUBSTITUTE>|<Substitute Covariate or N/A>|<Justification>\n",
    "                '''\n",
    "                # store in state\n",
    "                state['recent_response'] = issue\n",
    "                state['sending_agent'] = \"Statistician\"\n",
    "                state['predefined_next_agent'] = \"Clinician\"\n",
    "                state['predefined_next_question'] = issue\n",
    "                state['next'] = \"__end__\"\n",
    "                state['variables'][list_of_outcome[0]]['number_times_request_drop_covariates'] += 1\n",
    "                return state\n",
    "\n",
    "            # Step 4: Conduct Survival Analysis\n",
    "            results, plots, survival_str_info = conduct_survival_analysis(state, survival_methods, df, balanced_df, categorical_covariates, continuous_covariates, list_of_treatment, list_of_outcome, list_of_duration, outcome_definition, results, plots)\n",
    "\n",
    "            # results_str = f\"{balancing_str_info}\\n{survival_str_info}\"\n",
    "            results_str = f\"{survival_str_info}\"\n",
    "\n",
    "            # Making results for the specific outcome\n",
    "            state['variables'][list_of_outcome[0]]['trial_protocol'] = trial_protocol\n",
    "            state['variables'][list_of_outcome[0]]['balancing_information'] = balancing_str_info\n",
    "            state['variables'][list_of_outcome[0]]['balanced_df'] = balanced_df\n",
    "            state['variables'][list_of_outcome[0]]['subgroup_analysis_dict'] = {}\n",
    "            state['variables'][list_of_outcome[0]]['subgroup_analysis_dict']['No Stratification'] = results_str\n",
    "            state['variables'][list_of_outcome[0]]['categorical_covariates'] = categorical_covariates\n",
    "            state['variables'][list_of_outcome[0]]['continuous_covariates'] = continuous_covariates\n",
    "            state['variables'][list_of_outcome[0]]['results'] = results\n",
    "\n",
    "            # Check if Subgroup Analysis is needed\n",
    "            current_pval = None\n",
    "            if 'Cox Proportional Hazards Summary' in results:\n",
    "                current_pval = results['Cox Proportional Hazards Summary']['p'][list_of_treatment[0]]\n",
    "\n",
    "            if (current_pval is not None) and (current_pval >= 0.05):\n",
    "                # Results are not significant so we want to ask the clinican to ask for a stratification on a covariate\n",
    "                # Provide all the continuous covariates and their ranges\n",
    "                # Provide all the categorical covariates and their unique values\n",
    "                continuous_covariates_records = {}\n",
    "                for col in continuous_covariates:\n",
    "                    continuous_covariates_records[col] = [balanced_df[col].min(), balanced_df[col].max()]\n",
    "                categorical_covariates_records = {}\n",
    "                for col in categorical_covariates:\n",
    "                    categorical_covariates_records[col] = balanced_df[col].unique().tolist()\n",
    "                issue = f'''\n",
    "                Clinician, the current hazard ratio is not significant. Please provide a covariate to stratify on. Also provide the level to stratify on.\n",
    "\n",
    "                You are given the following covariates and their ranges:\n",
    "\n",
    "                Continuous Covariates:\n",
    "                {continuous_covariates_records}\n",
    "\n",
    "                Categorical Covariates:\n",
    "                {categorical_covariates_records}\n",
    "                \n",
    "                Expected Structure Output:\n",
    "                STRATIFY ON: <Covariate>\n",
    "                LEVEL: <Level>\n",
    "\n",
    "                The Covariate should be one of the covariates provided above. It should be spelled the EXACT same way as provided above.\n",
    "                Level should be a value within the range of the covariate. \\\n",
    "                Level **MUST** be a number. Do not output a placeholder or a string. It will be parsed and used for stratification.\n",
    "\n",
    "                Do not provide additional information or context in the response. No explanation is needed. Provide BOTH \"STRATIFY ON\" and \"LEVEL\" in the response.\n",
    "                ONLY provide **ONE** stratification.\n",
    "                '''\n",
    "                state['recent_response'] = issue\n",
    "                state['sending_agent'] = \"Statistician\"\n",
    "                state['predefined_next_agent'] = \"Clinician\"\n",
    "                state['predefined_next_question'] = issue\n",
    "                state['next'] = \"__end__\"\n",
    "                return state\n",
    "            else:\n",
    "                # If the results are significant, we can proceed to the next outcome\n",
    "                state['variables']['completed_outcomes'].append(list_of_outcome[0])\n",
    "                state['variables']['current_outcome_index'] += 1\n",
    "                state['recent_response'] = f\"Analysis for {list_of_outcome[0]} is completed. Proceeding to the next outcome.\"\n",
    "                state['ongoing_question'] = f\"Proceeding to the next outcome analysis.\"\n",
    "                state['next'] = \"Statistician\"\n",
    "\n",
    "\n",
    "    # Step 5: Write the full summary of results and context\n",
    "    state = _generate_final_report(state, llm)\n",
    "\n",
    "    if 'cnt_statistician_response' not in state['variables'] or state['variables']['cnt_statistician_response'] is None:\n",
    "        state['variables']['cnt_statistician_response'] = 0\n",
    "    else:\n",
    "        state['variables']['cnt_statistician_response'] += 1\n",
    "    \n",
    "    if state['variables']['cnt_statistician_response'] > 10:  # TODO: might change in the future\n",
    "        state[\"recent_response\"] += f\"\"\"The Statistician has reached the maximum number of response so that do not go back to Statistician again even being recommended by the others.\"\"\"\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the medical graph\n",
    "medical_builder = StateGraph(ClinicianState)\n",
    "medical_builder.add_node(Clinician)\n",
    "medical_builder.add_edge(START, \"Clinician\")\n",
    "medical_builder.add_conditional_edges(\"Clinician\", lambda state: state[\"next\"])\n",
    "medical_graph = medical_builder.compile()\n",
    "display(Image(medical_graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "# Build the statistician graph\n",
    "statistician_builder = StateGraph(StatisticianState)\n",
    "statistician_builder.add_node(Statistician)\n",
    "statistician_builder.add_edge(START, \"Statistician\")\n",
    "statistician_builder.add_conditional_edges(\"Statistician\", lambda state: state[\"next\"])\n",
    "statistician_graph = statistician_builder.compile()\n",
    "display(Image(statistician_graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "# Build the informatician graph (assuming it's a single function invoker)\n",
    "informatician_builder = StateGraph(InformaticianState)\n",
    "informatician_builder.add_node(Informatician)\n",
    "informatician_builder.add_edge(START, \"Informatician\")\n",
    "informatician_builder.add_conditional_edges(\"Informatician\", lambda state: state[\"next\"])\n",
    "informatician_graph = informatician_builder.compile()\n",
    "display(Image(informatician_graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "# Build the trial expert graph\n",
    "trial_builder = StateGraph(TrialistState)\n",
    "trial_builder.add_node(Trialist)\n",
    "trial_builder.add_edge(START, \"Trialist\")\n",
    "trial_builder.add_conditional_edges(\"Trialist\", lambda state: state[\"next\"])\n",
    "trial_graph = trial_builder.compile()\n",
    "display(Image(trial_graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "\n",
    "# Build node to invoke medical graph\n",
    "def ClinicianAgentInvoker(state: TopAgentState):\n",
    "    response = medical_graph.invoke({\"initial_question\": state[\"initial_question\"],\n",
    "                                     \"predefined_next_agent\": state[\"predefined_next_agent\"],\n",
    "                                     \"predefined_next_question\": state[\"predefined_next_question\"],\n",
    "                                     \"sending_agent\": state[\"sending_agent\"],\n",
    "                                     \"ongoing_question\": state[\"ongoing_question\"],\n",
    "                                     \"recent_response\": state[\"recent_response\"],\n",
    "                                     \"current_information\": state[\"current_information\"],\n",
    "                                     \"all_information\": state[\"all_information\"],\n",
    "                                     \"all_information_index\": state[\"all_information_index\"],\n",
    "                                     \"all_information_database\": state[\"all_information_database\"],\n",
    "                                     \"current_report\": state[\"current_report\"],\n",
    "                                     \"final_summary\": state[\"final_summary\"],\n",
    "                                     \"variables\": state[\"variables\"],\n",
    "                                     \"responses\": state[\"responses\"],\n",
    "                                     \"prompts\": state[\"prompts\"],\n",
    "                                     \"answering_agents\": state[\"answering_agents\"]})\n",
    "    state['predefined_next_agent'] = response['predefined_next_agent']\n",
    "    state['predefined_next_question'] = response['predefined_next_question']\n",
    "    state['sending_agent'] = response['sending_agent']\n",
    "    state['recent_response'] = response['recent_response']\n",
    "    state['final_summary'] = response['final_summary']\n",
    "    state['responses'] = response['responses']\n",
    "    state['prompts'] = response['prompts']\n",
    "    state['variables'] = response['variables']\n",
    "    return state\n",
    "\n",
    "# Build node to invoke statistician graph\n",
    "def StatisticianAgentInvoker(state: TopAgentState):\n",
    "    response = statistician_graph.invoke({\"initial_question\": state[\"initial_question\"],\n",
    "                                        \"predefined_next_agent\": state[\"predefined_next_agent\"],\n",
    "                                        \"predefined_next_question\": state[\"predefined_next_question\"],\n",
    "                                        \"sending_agent\": state[\"sending_agent\"],\n",
    "                                        \"ongoing_question\": state[\"ongoing_question\"],\n",
    "                                        \"recent_response\": state[\"recent_response\"],\n",
    "                                        \"current_information\": state[\"current_information\"],\n",
    "                                        \"all_information\": state[\"all_information\"],\n",
    "                                        \"all_information_index\": state[\"all_information_index\"],\n",
    "                                        \"all_information_database\": state[\"all_information_database\"],\n",
    "                                        \"final_summary\": state[\"final_summary\"],\n",
    "                                        \"variables\": state[\"variables\"],\n",
    "                                        \"responses\": state[\"responses\"],\n",
    "                                        \"prompts\": state[\"prompts\"],\n",
    "                                        \"answering_agents\": state[\"answering_agents\"]})\n",
    "    state['predefined_next_agent'] = response['predefined_next_agent']\n",
    "    state['predefined_next_question'] = response['predefined_next_question']\n",
    "    state['sending_agent'] = response['sending_agent']\n",
    "    state['recent_response'] = response['recent_response']\n",
    "    state['final_summary'] = response['final_summary']\n",
    "    if 'current_report' in response:\n",
    "        state['current_report'] = response['current_report']\n",
    "    state['responses'] = response['responses']\n",
    "    state['prompts'] = response['prompts']\n",
    "    state['variables'] = response['variables']\n",
    "    return state\n",
    "\n",
    "# Build node to invoke trial expert graph\n",
    "def TrialistInvoker(state: TopAgentState):\n",
    "    response = trial_graph.invoke({\"initial_question\": state[\"initial_question\"],\n",
    "                                        \"predefined_next_agent\": state[\"predefined_next_agent\"],\n",
    "                                        \"predefined_next_question\": state[\"predefined_next_question\"],\n",
    "                                        \"sending_agent\": state[\"sending_agent\"],\n",
    "                                        \"ongoing_question\": state[\"ongoing_question\"],\n",
    "                                        \"recent_response\": state[\"recent_response\"],\n",
    "                                        \"current_information\": state[\"current_information\"],\n",
    "                                        \"all_information\": state[\"all_information\"],\n",
    "                                        \"all_information_index\": state[\"all_information_index\"],\n",
    "                                        \"all_information_database\": state[\"all_information_database\"],\n",
    "                                        \"final_summary\": state[\"final_summary\"],\n",
    "                                        \"variables\": state[\"variables\"],\n",
    "                                        \"responses\": state[\"responses\"],\n",
    "                                        \"prompts\": state[\"prompts\"],\n",
    "                                        \"answering_agents\": state[\"answering_agents\"]})\n",
    "    state['predefined_next_agent'] = response['predefined_next_agent']\n",
    "    state['predefined_next_question'] = response['predefined_next_question']\n",
    "    state['sending_agent'] = response['sending_agent']\n",
    "    state['recent_response'] = response['recent_response']\n",
    "    state['final_summary'] = response['final_summary']\n",
    "    state['responses'] = response['responses']\n",
    "    state['prompts'] = response['prompts']\n",
    "    state['variables'] = response['variables']\n",
    "    return state\n",
    "\n",
    "def InformaticianAgentInvoker(state: TopAgentState):\n",
    "    response = informatician_graph.invoke({\"initial_question\": state[\"initial_question\"],\n",
    "                                        \"predefined_next_agent\": state[\"predefined_next_agent\"],\n",
    "                                        \"predefined_next_question\": state[\"predefined_next_question\"],\n",
    "                                        \"sending_agent\": state[\"sending_agent\"],\n",
    "                                        \"ongoing_question\": state[\"ongoing_question\"],\n",
    "                                        \"recent_response\": state[\"recent_response\"],\n",
    "                                        \"current_information\": state[\"current_information\"],\n",
    "                                        \"all_information\": state[\"all_information\"],\n",
    "                                        \"all_information_index\": state[\"all_information_index\"],\n",
    "                                        \"all_information_database\": state[\"all_information_database\"],\n",
    "                                        \"final_summary\": state[\"final_summary\"],\n",
    "                                        \"variables\": state[\"variables\"],\n",
    "                                        \"responses\": state[\"responses\"],\n",
    "                                        \"prompts\": state[\"prompts\"],\n",
    "                                        \"answering_agents\": state[\"answering_agents\"]})\n",
    "    state['predefined_next_agent'] = response['predefined_next_agent']\n",
    "    state['predefined_next_question'] = response['predefined_next_question']\n",
    "    state['sending_agent'] = response['sending_agent']\n",
    "    state['recent_response'] = response['recent_response']\n",
    "    state['final_summary'] = response['final_summary']\n",
    "    state['responses'] = response['responses']\n",
    "    state['prompts'] = response['prompts']\n",
    "    state['variables'] = response['variables']\n",
    "    return state\n",
    "\n",
    "# Build the top level graph\n",
    "top_members = [\"ClinicianAgentInvoker\", \"StatisticianAgentInvoker\", \"InformaticianAgentInvoker\", \"TrialistInvoker\"]\n",
    "top_builder = StateGraph(TopAgentState)\n",
    "top_builder.add_node(MainSupervisor)\n",
    "top_builder.add_node(ClinicianAgentInvoker)\n",
    "top_builder.add_node(StatisticianAgentInvoker)\n",
    "top_builder.add_node(TrialistInvoker)\n",
    "top_builder.add_node(InformaticianAgentInvoker)\n",
    "top_builder.add_edge(START, \"MainSupervisor\")\n",
    "for member in top_members:\n",
    "    top_builder.add_edge(member, \"MainSupervisor\")\n",
    "top_builder.add_conditional_edges(\"MainSupervisor\", lambda state: state[\"next\"])\n",
    "top_graph = top_builder.compile()\n",
    "display(Image(top_graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the initial state\n",
    "# Input\n",
    "if TRIAL_ID == \"NCT04134403\":\n",
    "    initial_input = {\"initial_question\": \"Design a trial emulation with proper protocols and eligibility criteria to determine the effectiveness of hydrocortisone in treating sepsis patients within the ICU.\\\n",
    "    Follow the steps broadly outlined below: \\\n",
    "    1. Get a trial to emulate from the Trialist. \\\n",
    "    2. Build the dataset for the trial emulation with the Informatician. \\\n",
    "    3. Conduct the trial emulation with the Statistician. \\\n",
    "    If there are any issues, the Clinician should be consulted.\",\n",
    "                    \"final_summary\": \"\",\n",
    "                    \"current_report\": \"\",\n",
    "                    \"ongoing_question\": \"\",\n",
    "                    \"current_information\": \"\",\n",
    "                    \"recent_response\": \"\",\n",
    "                    \"all_information\": \"\",\n",
    "                    \"all_information_index\": None,\n",
    "                    \"all_information_database\": None,\n",
    "                    \"predefined_next_agent\": \"\",\n",
    "                    \"predefined_next_question\": \"\",\n",
    "                    \"sending_agent\": \"\",\n",
    "                    \"variables\": {'trialid': 'NCT04134403'},\n",
    "                    \"responses\": [],\n",
    "                    \"prompts\": [],\n",
    "                    \"answering_agents\": []}\n",
    "    \n",
    "elif TRIAL_ID == \"NCT02856698\":\n",
    "    initial_input = {\"initial_question\": \"Determine the effectiveness of treating Acute Pulmonary Edema with midazolam.\\\n",
    "    Follow the steps broadly outlined below: \\\n",
    "    1. Get a trial to emulate from the Trialist. \\\n",
    "    2. Build the dataset for the trial emulation with the Informatician. \\\n",
    "    3. Conduct the trial emulation with the Statistician. \\\n",
    "    If there are any issues, the Clinician should be consulted.\",\n",
    "                    \"final_summary\": \"\",\n",
    "                    \"current_report\": \"\",\n",
    "                    \"ongoing_question\": \"\",\n",
    "                    \"current_information\": \"\",\n",
    "                    \"recent_response\": \"\",\n",
    "                    \"all_information\": \"\",\n",
    "                    \"all_information_index\": None,\n",
    "                    \"all_information_database\": None,\n",
    "                    \"predefined_next_agent\": \"\",\n",
    "                    \"predefined_next_question\": \"\",\n",
    "                    \"sending_agent\": \"\",\n",
    "                    \"variables\": {'trialid': 'NCT02856698'},\n",
    "                    \"responses\": [],\n",
    "                    \"prompts\": [],\n",
    "                    \"answering_agents\": []}\n",
    "\n",
    "elif TRIAL_ID == \"NCT06091982\":\n",
    "    initial_input = {\"initial_question\": \"Design a trial emulation to determine what is the effect of renal replacement therapy for severe acute kidney injury.\\\n",
    "    Follow the steps broadly outlined below: \\\n",
    "    Get a trial to emulate from the Trialist. \\\n",
    "    Build the dataset for the trial emulation with the Informatician. \\\n",
    "    Conduct the trial emulation with the Statistician. \\\n",
    "    If there are any issues, the Clinician should be consulted. \\\n",
    "    Do not assume the steps are done.\",\n",
    "                    \"final_summary\": \"\",\n",
    "                    \"current_report\": \"\",\n",
    "                    \"ongoing_question\": \"\",\n",
    "                    \"current_information\": \"\",\n",
    "                    \"recent_response\": \"\",\n",
    "                    \"all_information\": \"\",\n",
    "                    \"all_information_index\": None,\n",
    "                    \"all_information_database\": None,\n",
    "                    \"predefined_next_agent\": \"\",\n",
    "                    \"predefined_next_question\": \"\",\n",
    "                    \"sending_agent\": \"\",\n",
    "                    \"variables\": {'trialid': 'NCT06091982'},\n",
    "                    \"responses\": [],\n",
    "                    \"prompts\": [],\n",
    "                    \"answering_agents\": []}\n",
    "\n",
    "elif TRIAL_ID == \"NCT00475852\":\n",
    "    initial_input = {\"initial_question\": \"Design a trial emulation to determine what is the effect of nesiritide for heart failure.\\\n",
    "    Follow the steps broadly outlined below: \\\n",
    "    Get a trial to emulate from the Trialist. \\\n",
    "    Build the dataset for the trial emulation with the Informatician. \\\n",
    "    Conduct the trial emulation with the Statistician. \\\n",
    "    If there are any issues, the Clinician should be consulted. \\\n",
    "    Do not assume the steps are done.\",\n",
    "                    \"final_summary\": \"\",\n",
    "                    \"current_report\": \"\",\n",
    "                    \"ongoing_question\": \"\",\n",
    "                    \"current_information\": \"\",\n",
    "                    \"recent_response\": \"\",\n",
    "                    \"all_information\": \"\",\n",
    "                    \"all_information_index\": None,\n",
    "                    \"all_information_database\": None,\n",
    "                    \"predefined_next_agent\": \"\",\n",
    "                    \"predefined_next_question\": \"\",\n",
    "                    \"sending_agent\": \"\",\n",
    "                    \"variables\": {'trialid': 'NCT00475852'},\n",
    "                    \"responses\": [],\n",
    "                    \"prompts\": [],\n",
    "                    \"answering_agents\": []}\n",
    "    \n",
    "elif TRIAL_ID == \"NCT03872011\":\n",
    "    initial_input = {\"initial_question\": \"Design a trial emulation to determine what is the effect of steroids for septic shock.\\\n",
    "    Follow the steps broadly outlined below: \\\n",
    "    Get a trial to emulate from the Trialist. \\\n",
    "    Build the dataset for the trial emulation with the Informatician. \\\n",
    "    Conduct the trial emulation with the Statistician. \\\n",
    "    If there are any issues, the Clinician should be consulted. \\\n",
    "    Do not assume the steps are done.\",\n",
    "                    \"final_summary\": \"\",\n",
    "                    \"current_report\": \"\",\n",
    "                    \"ongoing_question\": \"\",\n",
    "                    \"current_information\": \"\",\n",
    "                    \"recent_response\": \"\",\n",
    "                    \"all_information\": \"\",\n",
    "                    \"all_information_index\": None,\n",
    "                    \"all_information_database\": None,\n",
    "                    \"predefined_next_agent\": \"\",\n",
    "                    \"predefined_next_question\": \"\",\n",
    "                    \"sending_agent\": \"\",\n",
    "                    \"variables\": {'trialid': 'NCT03872011'},\n",
    "                    \"responses\": [],\n",
    "                    \"prompts\": [],\n",
    "                    \"answering_agents\": []}\n",
    "    \n",
    "\n",
    "elif TRIAL_ID == \"NCT04691505\":\n",
    "    initial_input = {\"initial_question\": \"Design a trial emulation to determine what is the effect of Hydroxychloroquine for dementia.\\\n",
    "    Follow the steps broadly outlined below: \\\n",
    "    Get a trial to emulate from the Trialist. \\\n",
    "    Build the dataset for the trial emulation with the Informatician. \\\n",
    "    Conduct the trial emulation with the Statistician. \\\n",
    "    If there are any issues, the Clinician should be consulted. \\\n",
    "    Do not assume the steps are done.\",\n",
    "                    \"final_summary\": \"\",\n",
    "                    \"current_report\": \"\",\n",
    "                    \"ongoing_question\": \"\",\n",
    "                    \"current_information\": \"\",\n",
    "                    \"recent_response\": \"\",\n",
    "                    \"all_information\": \"\",\n",
    "                    \"all_information_index\": None,\n",
    "                    \"all_information_database\": None,\n",
    "                    \"predefined_next_agent\": \"\",\n",
    "                    \"predefined_next_question\": \"\",\n",
    "                    \"sending_agent\": \"\",\n",
    "                    \"variables\": {'trialid': 'NCT04691505', 'dataset': 'insight'},\n",
    "                    \"responses\": [],\n",
    "                    \"prompts\": [],\n",
    "                    \"answering_agents\": []}\n",
    "\n",
    "else:\n",
    "    assert False, \"Invalid Trial ID\"\n",
    "    \n",
    "# Thread\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}, \"recursion_limit\": 100}\n",
    "\n",
    "# Run the graph until the first interruption\n",
    "events = []\n",
    "for event in top_graph.stream(initial_input, thread, stream_mode=\"values\"):\n",
    "    events.append(event)\n",
    "print(\"\\n\")\n",
    "print(\"Final answer: \", event[\"final_summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final answer: \", event[\"final_summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_responses(responses):\n",
    "    summarized_responses = []\n",
    "    for response in responses:\n",
    "        summarize_llm_prompt = f'''\n",
    "        Summarize the following RESPONSE. Keep relevant details about what is being asked. Your summary will be used in a log.\n",
    "\n",
    "        RESPONSE: {response}\n",
    "\n",
    "        --------------------------\n",
    "        \n",
    "        The output must be in the following format:\n",
    "        <response>summarized response</response>\n",
    "\n",
    "        No extra explanations before or after.\n",
    "        '''\n",
    "\n",
    "        summarized_response = llm.invoke(summarize_llm_prompt, use_cache=False)\n",
    "        summarized_responses.append(summarized_response)\n",
    "    return summarized_responses\n",
    "\n",
    "summarized_responses = summarize_responses(events[-1]['responses'])\n",
    "\n",
    "# Conversation Log\n",
    "for agent, response in  zip(events[-1]['answering_agents'], summarized_responses):\n",
    "    print(f\"Agent: {agent}\")\n",
    "    print(\"\\n\")\n",
    "    print(f\"{response}\")\n",
    "    print(\"\\n\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
